{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0fb30d",
   "metadata": {},
   "source": [
    "# Project 4: **Build a Deep Research System**\n",
    "Welcome to project 4! For this project, we shift our focus from tool use and agents to *reasoning* models. You will practice state‚Äëof‚Äëthe‚Äëart inference‚Äëtime scaling methods such as *Chain‚Äëof‚ÄëThought* prompting and *Tree‚Äëof‚ÄëThoughts*, and briefly explore high-levels of training reasoning models using techniques like **STaR**.\n",
    "\n",
    "\n",
    "Finally, you will put everything together to build a *deep research agent* that can browse the web, reason over what it finds, and give structured answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54845369",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Apply common inference‚Äëtime scaling methods: **zero‚Äëshot / few‚Äëshot CoT, self‚Äëconsistency, sequential decoding, tree‚Äëof‚Äëthoughts**  \n",
    "* Gain intuition for **training** reasoning‚Äëcapable models following **STaR** approach \n",
    "* Build a minimal **deep‚Äëresearch agent** that combines step‚Äëby‚Äëstep reasoning with live web search   \n",
    "* Practice extending deep-search to a multi-agent system "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a40a86",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "1. Environment setup  \n",
    "2. Inference‚Äëtime scaling  \n",
    "   2.1 Few‚Äëshot & zero‚Äëshot‚ÄØCoT  \n",
    "   2.2 Self‚Äëconsistency\n",
    "   2.3 Sequential revisions  \n",
    "   2.4 Tree‚Äëof‚ÄëThought\n",
    "3. STaR for training models for reasoning  \n",
    "4. Deep-research agent  \n",
    "5. (Optional) Multi-agent deep-research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e480f76",
   "metadata": {},
   "source": [
    "# 1‚Äë Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c2218",
   "metadata": {},
   "source": [
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yaml && conda activate deep_research\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=deep_research --display-name \"deep_research\"\n",
    "```\n",
    "Once this is done, you can select \"deep_research\" from the Kernel ‚Üí Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project we use the `llama3.2:3b` and `deepseek-r1:8b` models. You can try other smaller or larger reasoning LLMs such as `qwen2.5:3b-instruct` or `phi4-mini` to compare performance. Explore available models here: https://ollama.com/library.\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "ollama pull deepseek-r1:8b\n",
    "# Additional small reasoning models to compare\n",
    "# ollama pull qwen2.5:3b-instruct\n",
    "# ollama pull phi4-mini\n",
    "\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8d1b7",
   "metadata": {},
   "source": [
    "---  \n",
    "# 2‚Äë Inference‚Äëtime scaling\n",
    "\n",
    "Inference-time scaling refers to techniques that make an existing model reason better without retraining it. Instead of changing the model‚Äôs weights, we achieve reasoning capability by adjusting how we prompt, sample, or aggregate LLM's outputs.\n",
    "\n",
    "In this section, we‚Äôll explore several inference-time strategies that improve reasoning quality using a non-reasoning base model. You will experiment with and compare methods such as:\n",
    "\n",
    "- Few-shot Chain-of-Thought (CoT)\n",
    "- Zero-shot CoT\n",
    "- Self-consistency\n",
    "- Sequential revision\n",
    "- Tree-of-Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d499a",
   "metadata": {},
   "source": [
    "### 2.1: Few‚ÄëShot CoT\n",
    "Few-shot prompting helps a model reason by showing one or multiple examples before asking a new question. By observing the pattern of reasoning and final answers, the model learns how to structure its own reasoning process on the new input.\n",
    "\n",
    "In this exercise, you will create a prompt that includes a few example Q&A pairs demonstrating step-by-step reasoning. Then, you will feed a new question and see the model‚Äôs output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "173d73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the nth term of an arithmetic sequence, we use the formula:\n",
      "\n",
      "an = a1 + (n - 1)d\n",
      "\n",
      "where:\n",
      "a1 = the first term (2 in this case)\n",
      "d = the common difference (1 in this case)\n",
      "n = the term number\n",
      "\n",
      "Since you want to know the 4th term, plug in n = 4:\n",
      "\n",
      "a4 = 2 + (4 - 1) * 1\n",
      "= 2 + 3 * 1\n",
      "= 2 + 3\n",
      "= 5\n",
      "\n",
      "So the 4th number in the sequence is 5.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Write a few examples showing reasoning steps\n",
    "# Step 2: Write your new question\n",
    "# Step 3: Concatenate examples + new question into a single prompt\n",
    "# Step 4: Call your Ollama or OpenAI client to get a response from llama3.2:3b # e.g., client.chat.completions.create(...)\n",
    "# Step 5: Print the final answer\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\"\n",
    "EXAMPLES = \"\"\"\n",
    "Q: What is the 9th Fibonacci number?\n",
    "A: The Fibonacci sequence starts with 0 and 1. Each next number is found by adding the previous 2 numbers. So the sequence goes:\n",
    "* 0\n",
    "* 1\n",
    "* 0 + 1 = 1\n",
    "* 1 + 1 = 2\n",
    "* 1 + 2 = 3\n",
    "* 2 + 3 = 5\n",
    "* 3 + 5 = 8\n",
    "* 5 + 8 = 13\n",
    "* 8 + 13 = 21\n",
    "\n",
    "So the 9th Fibonacci number is 21.\n",
    "\"\"\"\n",
    "\n",
    "USER_QUESTION = \"\"\"\n",
    "Q: What is the 4th number in an arithmetic sequence with a common difference of 1 and a first term of 2?\n",
    "A: \"\"\"\n",
    "\n",
    "\n",
    "llm = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "\n",
    "response = llm.chat.completions.create(\n",
    "    model=\"llama3.2:3b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": EXAMPLES},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION},\n",
    "        ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e29d9",
   "metadata": {},
   "source": [
    "### (Optional) Few-shot CoT on GPT2\n",
    "GPT-2 is a pre-trained language model without instruction tuning. It continues text rather than answering questions. In this section, you'll try the exact same CoT pattern on GPT-2 and observe what happens. The goal is to test whether few-shot CoT alone can elicit structured reasoning from a non-chat LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8af711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '\\nQ: What is the 9th Fibonacci number?\\nA: The Fibonacci sequence starts with 0 and 1. Each next number is found by adding the previous 2 numbers. So the sequence goes:\\n* 0\\n* 1\\n* 0 + 1 = 1\\n* 1 + 1 = 2\\n* 1 + 2 = 3\\n* 2 + 3 = 5\\n* 3 + 5 = 8\\n* 5 + 8 = 13\\n* 8 + 13 = 21\\n\\nSo the 9th Fibonacci number is 21.\\n\\nQ: What is the 4th number in an arithmetic sequence with a common difference of 1 and a first term of 2?\\nA: 0000000000000000\\n\\nQ: What is the 5th Fibonacci number?\\n\\nA: '},\n",
       " {'generated_text': '\\nQ: What is the 9th Fibonacci number?\\nA: The Fibonacci sequence starts with 0 and 1. Each next number is found by adding the previous 2 numbers. So the sequence goes:\\n* 0\\n* 1\\n* 0 + 1 = 1\\n* 1 + 1 = 2\\n* 1 + 2 = 3\\n* 2 + 3 = 5\\n* 3 + 5 = 8\\n* 5 + 8 = 13\\n* 8 + 13 = 21\\n\\nSo the 9th Fibonacci number is 21.\\n\\nQ: What is the 4th number in an arithmetic sequence with a common difference of 1 and a first term of 2?\\nA: \\xa0The 4th Fibonacci number is the first term of 2. \\xa0Each next number'},\n",
       " {'generated_text': '\\nQ: What is the 9th Fibonacci number?\\nA: The Fibonacci sequence starts with 0 and 1. Each next number is found by adding the previous 2 numbers. So the sequence goes:\\n* 0\\n* 1\\n* 0 + 1 = 1\\n* 1 + 1 = 2\\n* 1 + 2 = 3\\n* 2 + 3 = 5\\n* 3 + 5 = 8\\n* 5 + 8 = 13\\n* 8 + 13 = 21\\n\\nSo the 9th Fibonacci number is 21.\\n\\nQ: What is the 4th number in an arithmetic sequence with a common difference of 1 and a first term of 2?\\nA: \\xa0The 4-digit number is a common number, and the last 4 digits in the sequence are'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load GPT-2 text-generation from huggingface (https://huggingface.co/docs/transformers/en/model_doc/gpt2)\n",
    "# Step 2: Write 1‚Äì2 few-shot reasoning examples (short, explicit steps + final answer in your own unique format)\n",
    "# Step 3: Append a new test question after the examples to form one prompt string\n",
    "# Step 4: Generate 1‚Äì3 completions with different decoding settings (e.g., greedy vs. top-k)\n",
    "# Step 5: Print raw outputs; check if steps are followed and if the final answer is correct\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~12-15 lines of code)\n",
    "\"\"\"\n",
    "pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", dtype=torch.float16, device=0)\n",
    "pipeline(EXAMPLES + USER_QUESTION, max_new_tokens=20, do_sample=True, temperature=0.7, top_k=10, num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adee0e7",
   "metadata": {},
   "source": [
    "### 2.2: Zero‚ÄëShot Chain‚Äëof‚ÄëThought\n",
    "Zero-shot CoT encourages the model to reason without examples by adding a short cue such as ‚ÄúLet‚Äôs think step by step.‚Äù This simple phrase often activates the model‚Äôs latent reasoning ability even when no demonstrations are provided. It serves as a baseline to compare with few-shot and other inference-time scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c444eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fourth number in an arithmetic sequence with a first term of 2 and a common difference of 1 can be found using the formula for the nth term:\n",
      "\n",
      "a_n = a_1 + (n - 1) * d\n",
      "\n",
      "where:\n",
      "- a_1 = 2 (first term)\n",
      "- d = 1 (common difference)\n",
      "- n = 4 (term to find)\n",
      "\n",
      "Substitute the values into the formula:\n",
      "\n",
      "a‚ÇÑ = 2 + (4 - 1) * 1  \n",
      "a‚ÇÑ = 2 + (3) * 1  \n",
      "a‚ÇÑ = 2 + 3  \n",
      "a‚ÇÑ = 5\n",
      "\n",
      "This result can be verified by listing out each term sequentially:\n",
      "- First term: a‚ÇÅ = 2\n",
      "- Second term: a‚ÇÇ = a‚ÇÅ + d = 2 + 1 = 3\n",
      "- Third term: a‚ÇÉ = a‚ÇÇ + d = 3 + 1 = 4\n",
      "- Fourth term: a‚ÇÑ = a‚ÇÉ + d = 4 + 1 = 5\n",
      "\n",
      "Thus, the fourth number is 5.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Step 1: Write the question and a zero-shot CoT cue (e.g., \"Let's think step by step.\")\n",
    "# Step 2: Build a single prompt string that includes brief role guidance plus the question\n",
    "# Step 3: Call your Ollama or OpenAI client to get a response from llama3.2:3b  # e.g., client.chat.completions.create(...)\n",
    "# Step 4: Print the chain and the final answer\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~10 lines of code)\n",
    "\"\"\"\n",
    "response = llm.chat.completions.create(\n",
    "    # model=\"llama3.2:3b\",\n",
    "    model=\"deepseek-r1:8b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION + \"Let's think step by step.\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686708da",
   "metadata": {},
   "source": [
    "### 2.3 Self‚ÄëConsistency\n",
    "Self-consistency enhances reasoning accuracy by sampling multiple independent reasoning paths for the same question instead of relying on a single deterministic answer. Each run may follow a slightly different logical chain, and the diversity helps correct individual mistakes. After generating several reasoning traces, you then aggregate the final answers using majority voting.\n",
    "\n",
    "This approach is especially useful when tasks involve multi-step reasoning or arithmetic, where single-path outputs may be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2fb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "To find the square root of 144, we need to determine the number that multiplies itself to give us 144.\n",
      "\n",
      "Step 1: Start with a perfect square like 12¬≤ = 144.\n",
      "\n",
      "Step 2: Since we have found our starting point, 144 equals (12) squared. We can say our answer is 12 because it‚Äôs equal to (square root of (12)) squared.\n",
      "\n",
      "Therefore, the square root of 144 is 12 .\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we need to determine the number that multiplied by itself equals 144.\n",
      "\n",
      "Step 1: We know that 12 * 12 = 144.\n",
      "\n",
      "Step 2: Therefore, the square root of 144 is 12, because 12 multiplied by itself results in 144.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we can start by looking for perfect squares that are close to 144.\n",
      "\n",
      "We know that:\n",
      "\n",
      "* 11^2 = 121 (which is less than 144)\n",
      "* 12^2 = 144 (which is exactly 144!)\n",
      "* 13^2 = 169 (which is greater than 144)\n",
      "\n",
      "Since 12^2 equals 144, we can conclude that the square root of 144 is:\n",
      "\n",
      "‚àö144 = 12\n",
      "\n",
      "So, the answer is: ‚àö144 = 12.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we can start by asking ourselves what number multiplied by itself gives us 144.\n",
      "\n",
      "Let x be the square root of 144. Then, x squared equals 144:\n",
      "\n",
      "x¬≤ = 144\n",
      "\n",
      "We can try squaring some numbers to see which one gets us back to 144. For example, let's try 12 and 13:\n",
      "\n",
      "12¬≤ = 144 (yes!)\n",
      "\n",
      "So, we have found that the square root of 144 is 12.\n",
      "\n",
      "Answer: The correct answer is A) 12!\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we need to determine what number multiplied by itself gives us 144.\n",
      "\n",
      "Let's start by finding the prime factors of 144:\n",
      "\n",
      "144 = 2 √ó 72\n",
      "= 2 √ó 2 √ó 36\n",
      "= 2 √ó 2 √ó 2 √ó 18\n",
      "= 2 √ó 2 √ó 2 √ó 2 √ó 9\n",
      "= 2^4 √ó 3^2\n",
      "\n",
      "Now, we can see that the prime factors of 144 are 2 (four times) and 3 (twice). To find the square root, we take half of each exponent:\n",
      "\n",
      "‚àö144 = ‚àö(2^4 √ó 3^2)\n",
      "= 2^2 √ó 3\n",
      "= 4 √ó 3\n",
      "= 12\n",
      "\n",
      "Therefore, the square root of 144 is 12.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we need to determine what number multiplied by itself gives us 144.\n",
      "\n",
      "Let's denote the square root as ‚àöx = x. We can rewrite this equation as:\n",
      "\n",
      "x * x = 144\n",
      "\n",
      "This simplifies to:\n",
      "x^2 = 144\n",
      "\n",
      "Now, we need to find a whole number that when squared equals 144. \n",
      "\n",
      "By inspection or using a calculator, we find that:\n",
      "12 * 12 = 144\n",
      "\n",
      "So, the square root of 144 is ‚àö144 = 12.\n",
      "\n",
      "Answer: The square root of 144 is 12.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we need to determine the number that, when multiplied by itself, gives us 144.\n",
      "\n",
      "Let's start by looking for perfect squares around 144:\n",
      "\n",
      "* The perfect square before 144 is 121 (11¬≤)\n",
      "* The perfect square after 144 is not immediately apparent\n",
      "\n",
      "Notice that 12¬≤ = 144. So, we can say that the square root of 144 is ... .\n",
      "\n",
      "Is there something you'd like to know about ‚àö144 or would you like me to proceed?\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we need to determine the number that, when multiplied by itself, equals 144.\n",
      "\n",
      "Here are the steps:\n",
      "\n",
      "1. Look for perfect squares: Recognize that 144 is a perfect square (12¬≤ = 144).\n",
      "2. Identify the square root: Since 144 is a perfect square, its square root is simply the value that was multiplied to get 144, which is 12.\n",
      "3. Check (optional): To confirm, we can square the result (12) to make sure it equals 144 (12¬≤ = 144).\n",
      "\n",
      "Therefore, the square root of 144 is 12.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we need to look for a number that multiplied by itself gives us 144.\n",
      "\n",
      "Step 1: Start with perfect squares in mind\n",
      "The perfect squares around 144 are 121 (11^2) and 169 (13^2).\n",
      "\n",
      "Step 2: Check the squares between 121 and 169\n",
      "Upon checking, we see that 12^2 = 144.\n",
      "\n",
      "Therefore, the square root of 144 is 12.\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "To find the square root of 144, we need to determine a number that when multiplied by itself equals 144.\n",
      "\n",
      "Step 1: Look for perfect squares around 144.\n",
      "We know that 12 x 12 = 144.\n",
      "\n",
      "Step 2: Check if there are any other perfect squares close to 144.\n",
      "There aren't any other perfect squares close to 144, so we can confidently say the square root of 144 is 12.\n",
      "\n",
      "The final answer is ‚àö144 = 12.\n",
      "--------------------------------\n",
      "====================================================================================================\n",
      "Votes: 8\n",
      "Chosen answer: 12\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re, collections, string\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def cot_answer(question, temperature=1.0):\n",
    "    # Generate a step-by-step reasoning chain for the given question and extract the final answer.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~10 lines of code)\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question + \"\\nA: Let's think step by step.\"},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def self_consistent(question, n=10):\n",
    "    # Run multiple reasoning chains and select the most frequent final answer by majority voting.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~12 lines of code)\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    response_counts = {}\n",
    "    greatest_key = None\n",
    "    for _ in range(n):\n",
    "        response = cot_answer(question, 1.0)\n",
    "        print(\"--------------------------------\")\n",
    "        print(response)\n",
    "        print(\"--------------------------------\")\n",
    "        responses.append(response)\n",
    "        # take the last 4 characters of the response, remove punctuation and whitespace and add\n",
    "        # it to a hashmap of response -> count.  If the current value is greater than the greatest so far,\n",
    "        # replace it with the current value and replace the greatest key with the current key.\n",
    "        last_4 = response[-4:]\n",
    "        last_4 = last_4.translate(str.maketrans('', '', string.punctuation))\n",
    "        last_4 = last_4.strip()\n",
    "        response_counts[last_4] = response_counts.get(last_4, 0) + 1\n",
    "        if response_counts[last_4] > response_counts.get(greatest_key, 0):\n",
    "            greatest_key = last_4\n",
    "    # winner, counter = collections.Counter(responses).most_common(1)[0]\n",
    "    winner, counter = greatest_key, response_counts[greatest_key]\n",
    "    return winner, counter\n",
    "\n",
    "\n",
    "\n",
    "question = \"What is the square root of 144?\"\n",
    "winner, counter = self_consistent(question)\n",
    "print(\"=\"*100)\n",
    "print(\"Votes:\", counter)\n",
    "print(\"Chosen answer:\", winner)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bea715",
   "metadata": {},
   "source": [
    "### 2.4: Sequential Revision\n",
    "\n",
    "Sequential revision iteratively improves an answer by generating a first draft, critiquing it, and producing revised drafts that condition on prior answers. Each round should be short and focused, so improvements accumulate without drifting from the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07e5859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "To find the square root of 144, we can start by thinking about what a perfect square is. A perfect square is a number that can be expressed as the product of an integer multiplied by itself.\n",
      "\n",
      "For example, some perfect squares are:\n",
      "\n",
      "- 1 = 1 √ó 1\n",
      "- 4 = 2 √ó 2\n",
      "- 9 = 3 √ó 3\n",
      "\n",
      "Now, let's try to find two integers whose product is 144. If we start multiplying numbers, we can see that:\n",
      "\n",
      "- 12 √ó 12 = 144\n",
      "\n",
      "Wow, it looks like 12 multiplied by itself gives us 144!\n",
      "\n",
      "Therefore, the square root of 144 is... \n",
      "\n",
      "‚àö144 = 12\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "The correct answer is: ‚àö144 = 12\n",
      "\n",
      "However, to complete the explanation, we can also express this as:\n",
      "\n",
      "Since 12 multiplied by itself results in 144, and the reverse operation (taking the inverse or finding the \"square root\") is needed to go back to just 12, it's more conventional to write the square root of a number as both its positive value and its negative equivalent.\n",
      "\n",
      "Therefore, another way to express it would be:\n",
      "\n",
      "‚àö144 = ¬±12,\n",
      "--------------------------------\n",
      "That's correct! I'll make sure to revise my previous response with the complete explanation. Here is the revised answer:\n",
      "\n",
      "‚àö144 = 12\n",
      "\n",
      "Since 12 multiplied by itself results in 144, and the reverse operation (taking the inverse or finding the \"square root\") is needed to go back to just 12, it's more conventional to write the square root of a number as both its positive value and its negative equivalent.\n",
      "\n",
      "Therefore, another way to express it would be:\n",
      "\n",
      "‚àö144 = ¬±12\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def sequential_revision(question: str, max_steps: int = 3) -> str:\n",
    "    # Generate an initial draft answer, then iteratively refine it by conditioning each revision on the previous one.\n",
    "    # Step 1: Ask the model to produce the first draft for the given question\n",
    "    # Step 2: Loop for max_steps-1 times, each time feeding the last draft back to the model with a request to revise\n",
    "    # Step 3: Print each draft to observe how the answer evolves\n",
    "    # Step 4: Return the final improved draft\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~20 lines of code)\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question + \"\\nA: Let's think step by step.\"},\n",
    "        ],\n",
    "    )\n",
    "    for _ in range(max_steps - 1):\n",
    "        last_response = response.choices[0].message.content\n",
    "        print(\"--------------------------------\")\n",
    "        print(last_response)\n",
    "        print(\"--------------------------------\")\n",
    "        response = client.chat.completions.create(\n",
    "            model = MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that revises answers.  Here's the last answer: \"},\n",
    "                {\"role\": \"user\", \"content\": last_response},\n",
    "            ],\n",
    "        )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "# Step 1: Define a question that benefits from multi-step reasoning\n",
    "# Step 2: Call sequential_revision(question, max_steps)\n",
    "# Step 3: Print the final output\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\"\n",
    "print(sequential_revision(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9319ee8",
   "metadata": {},
   "source": [
    "### 2.5 Tree‚Äëof‚ÄëThoughts\n",
    "Tree-of-Thoughts reframes reasoning as a search process rather than a single forward chain.\n",
    "Instead of producing one linear sequence of thoughts, the model generates multiple candidate thoughts at each step, evaluates their promise, and then expands only the best few. This allows exploration of different reasoning paths before committing to a final answer, similar to how humans brainstorm, prune, and refine ideas.\n",
    "\n",
    "\n",
    "In this section, you‚Äôll experiment with two simplified versions of ToT:\n",
    "1. Word Ladder puzzle solver: a small example where each ‚Äúthought‚Äù is a candidate word transition.\n",
    "2. Generic ToT search (depth 2, width 2): a minimal logic to expand, evaluate, and select reasoning branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d047801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_frontier [['hit']]\n",
      "old_frontier [['hit', 'hot'], ['hit', 'lit']]\n",
      "old_frontier [['hit', 'hot', 'dot'], ['hit', 'hot', 'lot']]\n",
      "old_frontier [['hit', 'hot', 'dot', 'dog'], ['hit', 'hot', 'lot', 'log']]\n",
      "['hit', 'hot', 'dot', 'dog', 'cog']\n"
     ]
    }
   ],
   "source": [
    "###### Word Ladder Puzzle ##########\n",
    "import string\n",
    "\n",
    "\n",
    "def neighbors(word, vocabulary):\n",
    "    # Generate all valid one-letter mutations of 'word' that exist in 'vocabulary' and return them.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~6-8 lines)\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "    for i in range(len(word)):\n",
    "        for c in string.ascii_lowercase:\n",
    "            if c != word[i]:\n",
    "                neighbor = word[:i] + c + word[i+1:]\n",
    "                if neighbor in vocabulary:\n",
    "                    neighbors.append(neighbor)\n",
    "    return neighbors\n",
    "\n",
    "def tree_of_thought(start, goal, vocab, max_depth=5, beam_width=4):\n",
    "    # Search over partial thoughts (paths) using a small beam.\n",
    "    # Step 1: Initialize the frontier with a single path [start]\n",
    "    # Step 2: For each depth, expand each path by one neighbor from 'neighbors'\n",
    "    # Step 3: Score paths by edit distance between last word and 'goal' (smaller is better)\n",
    "    # Step 4: Keep the top 'beam_width' paths and stop early if any reaches 'goal'\n",
    "    # Step 5: Return the best goal-reaching path or None\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~14-18 lines)\n",
    "    \"\"\"\n",
    "    old_frontier = [[start]]\n",
    "    already_searched = set()\n",
    "    already_searched.add(start)\n",
    "    for _ in range(max_depth):\n",
    "        print(\"old_frontier\", old_frontier)\n",
    "        new_frontier = []\n",
    "        for path in old_frontier:\n",
    "            neighbs = neighbors(path[-1], vocab)\n",
    "            for nb in neighbs:\n",
    "                if nb in already_searched:\n",
    "                    continue\n",
    "                already_searched.add(nb)\n",
    "                new_frontier.append(path + [nb])\n",
    "\n",
    "        scores = {}\n",
    "        for path in new_frontier:\n",
    "            edit_distance = sum(1 for a, b in zip(path[-1], goal) if a != b)\n",
    "            if edit_distance == 0:\n",
    "                return path\n",
    "            scores[\"~\".join(path)] = edit_distance\n",
    "        old_frontier = sorted(new_frontier, key=lambda x: scores[\"~\".join(x)])[:beam_width]\n",
    "    print(\"old_frontier\", old_frontier)\n",
    "    return None\n",
    "\n",
    "\n",
    "vocab = {\"hit\",\"dot\",\"cog\",\"log\",\"dog\",\"lot\",\"lit\",\"hot\"}\n",
    "print(tree_of_thought(\"hit\", \"cog\", vocab)) # one candidate solution: ['hit', 'hot', 'dot', 'dog', 'cog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89067302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "7\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "6\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "8\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "8\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "8\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "8\n",
      "--------------------------------\n",
      "Best solution (score 8):\n",
      "Here is the revised plan:\n",
      "\n",
      "**Weekend Science Workshop Plan**\n",
      "\n",
      "**Workshop Title:** \"Curious Minds: Exploring the Wonders of Science\"\n",
      "\n",
      "**Age Group:** 12-year-olds (grades 7-8)\n",
      "\n",
      "**Duration:** Saturday and Sunday, 9am - 4pm\n",
      "\n",
      "**Objective:** To provide an immersive and engaging experience that sparks curiosity and interest in science among young participants.\n",
      "\n",
      "**Agenda:**\n",
      "\n",
      "**Saturday (9:00 am - 4:00 pm)**\n",
      "\n",
      "1. **Welcome and Icebreaker (9:00 am - 9:30 am)**\n",
      "\t* Introduce the workshop leaders and facilitators.\n",
      "\t* Fun icebreaker activity to get students comfortable with each other.\n",
      "2. **Introduction to Science and Engineering Design Thinking (9:30 am - 10:30 am)**\n",
      "\t* Overview of basic scientific principles and practices.\n",
      "\t* Introduction to engineering design thinking methods.\n",
      "3. **Experimentation Station I: Materials Science (10:30 am - 12:00 pm)**\n",
      "\t* Hands-on activity exploring the properties of various materials (e.g., magnets, strings, and foams).\n",
      "4. **Lunch Break (12:00 pm - 1:00 pm)**\n",
      "\n",
      "**Sunday (9:00 am - 4:00 pm)**\n",
      "\n",
      "1. **Experimentation Station II: Electricity and Electronics (9:00 am - 10:30 am)**\n",
      "\t* Hands-on activity exploring the basics of electricity and simple circuits.\n",
      "2. **Robotics Workshop (10:30 am - 12:00 pm)**\n",
      "\t* Guided workshop using a robotic kit to build and program basic robots.\n",
      "3. **Lunch Break (12:00 pm - 1:00 pm)**\n",
      "\n",
      "**Experimentation Station III: Environmental Science (1:00 pm - 2:30 pm)**\n",
      "\t* Hands-on activity exploring the impact of human activities on the environment.\n",
      "\n",
      "**Wrap-up and Evaluation (2:30 pm - 4:00 pm)**\n",
      "\n",
      "1. **Student Presentations**\n",
      "\t* Students will share their findings from each experimentation station.\n",
      "\t* Encourages critical thinking, communication skills, and collaboration.\n",
      "2. **Feedback and Closing Ceremony**\n",
      "\n",
      "**Materials Needed:**\n",
      "\n",
      "* Various materials for experimentation stations (e.g., magnets, foams, sticks, strings)\n",
      "* Robotics kits\n",
      "* Electricity-related components (e.g., resistors, capacitors, wires)\n",
      "* Environmental science samples and equipment (e.g., plants, soil, water)\n",
      "* Protective gear (gloves, goggles, etc.)\n",
      "* Whiteboard and markers for presentations\n",
      "\n",
      "**Workshop Leader Roles:**\n",
      "\n",
      "1. **Lead Scientists**: Responsible for facilitating experiments, ensuring student safety, and providing guidance throughout the workshop.\n",
      "2. **Assistant Facilitators**: Assist with setup, supervise students during each experiment, and provide additional support if needed.\n",
      "\n",
      "**Safety Precautions:**\n",
      "\n",
      "* Ensure all materials are safely handled and used, following proper protocols for waste disposal and health risks (e.g., chemicals, electricity).\n",
      "* Provide necessary personal protective equipment (PPE) for each activity.\n",
      "* Establish an emergency response plan in case of accidents or injuries.\n",
      "\n",
      "**Additional Ideas to Enhance the Workshop:**\n",
      "\n",
      "1. **Guest Lecture**: Invite a local scientist or engineer to give a talk on their area of expertise.\n",
      "2. **Maker Faire**: Plan a mini Maker Faire where students can showcase their projects and share their creations with peers.\n",
      "3. **Collaborative Projects**: Encourage students to work in teams to design and build their own projects using materials from previous experimentation stations.\n",
      "\n",
      "By incorporating these additional elements, the workshop will become even more engaging, interactive, and memorable for the young participants!\n"
     ]
    }
   ],
   "source": [
    "###### Generic ToT Search ##########\n",
    "\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "client = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "def propose_thoughts(question, state, k=2):\n",
    "    # Propose up to k next ‚Äúthoughts‚Äù that extend the current partial solution/state.\n",
    "    # Steps: build a short prompt with problem + current state; call your client with n=k. Then return a list of stripped strings (‚â§ k).\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _ in range(k):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": question + \"\\n\" + state}],\n",
    "            temperature=1.0,\n",
    "        )\n",
    "        results.append(response.choices[0].message.content.strip())\n",
    "    return results\n",
    "\n",
    "\n",
    "def score_state(question, state):\n",
    "    # Score how promising a partial solution is on a 1‚Äì10 scale (higher is better).\n",
    "    # Steps: build a rating prompt; call the model; parse the first integer 1‚Äì10;\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-10 lines)\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that scores the quality of a partial solution to a problem on a scale of 1 to 10.  Only return the score, no other text.  Here's the problem and the current state:\"},\n",
    "            {\"role\": \"user\", \"content\": question + \"\\n\" + state},\n",
    "        ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    print(\"--------------------------------\")\n",
    "    print(result)\n",
    "    print(\"--------------------------------\")\n",
    "    return int(result)\n",
    "\n",
    "\n",
    "def tree_of_thoughts(question, depth=2, width=2):\n",
    "    # Run a tiny ToT search: expand states with propose_thoughts, score with score_state, keep top-k at each depth.\n",
    "    # Steps: initialize frontier=[(\"\", 0)]; for each depth, expand each state with k=width thoughts; score each; sort by score desc; keep top 'width'; return best state and score.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~12-16 lines)\n",
    "    \"\"\"\n",
    "    old_frontier = [(\"\", 0)]\n",
    "    for _ in range(depth):\n",
    "        new_frontier = []\n",
    "        for thought_node, score in old_frontier:\n",
    "            new_thought_nodes = propose_thoughts(question, thought_node, width)\n",
    "            new_frontier.extend([(new_thought_node, score_state(question, new_thought_node)) for new_thought_node in new_thought_nodes])\n",
    "        old_frontier = sorted(new_frontier, key=lambda x: x[1], reverse=True)[:width]\n",
    "    return old_frontier[0]\n",
    "\n",
    "question = \"Design a plan for a weekend science workshop for 12-year-olds.\"\n",
    "solution, score = tree_of_thoughts(question)\n",
    "\n",
    "print(f\"Best solution (score {score}):\\n{solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc38f6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 3‚Äë Training Models for Reasoning\n",
    "\n",
    "### 3.1: CoT Training\n",
    "Chain-of-Thought (CoT) training conditions the model on explicit rationales during fine-tuning. Instead of teaching the model to output only the final answer, we train on (question, rationale, answer) so the model learns to internalize multi-step reasoning patterns. A practical recipe is STaR (Self-Taught Reasoner), which uses a stronger teacher model to bootstrap rationales that a smaller student can learn from.\n",
    "\n",
    "For tasks that require multi-hop reasoning, models fine-tuned on rationales often achieve higher accuracy and are more stable at inference time than models trained on direct answers only. \n",
    "\n",
    "Training a full language model is beyond the scope of this notebook, but here is the high-level workflow followed by a short pseudocode:\n",
    "- Collect questions: Prepare a dataset of questions and correct answers.\n",
    "- Generate rationales: Use a strong LLM to produce step-by-step reasoning ending with the correct answer.\n",
    "- Filter and clean: Discard incorrect or low-quality rationales.\n",
    "- Prepare training data: Format triples (question, rationale, answer) for supervised fine-tuning.\n",
    "- Fine-tune: Fine-tune the LLM on rationales.\n",
    "- Iterate: Refine prompts, improve data quality, and retrain for stronger reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode (STaR loop)\n",
    "# for round in 1 ... iters:\n",
    "    # STEP 1: self-generate reasoning (teacher creates rationale + answer)\n",
    "    # STEP 2: keep only correct, high-quality traces\n",
    "    # STEP 3: fine-tune student on (question, rationale, answer) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b53c70",
   "metadata": {},
   "source": [
    "### 3.2: ORM¬†vs¬†PRM¬†+ RL\n",
    "Training a Reward Model (RM) allows large language models to be improved through reinforcement learning (RL). Instead of fine-tuning directly on examples, we train a separate model that can score or rank model outputs, and use those scores as feedback signals to refine the policy model.\n",
    "\n",
    "Two main reward modeling approaches are ORM (predicts a scalar reward for the final answer) and PRM (evaluates the reasoning steps instead of just the outcome)\n",
    "\n",
    "\n",
    "\n",
    "| Approach | Typical loss | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "|*Outcome Reward Model* | Predict scalar reward | Easy to collect training data using verifiers |\n",
    "|*Process Reward Model* | Predict rewards per step | Difficult to collect training data but more accurate |\n",
    "| *RLHF* | Use RM as reward in **RL** fine‚Äëtuning | Aligns policy with human signals | Aligns model policy with human or synthetic preferences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for round = 1 ... iters:\n",
    "    # STEP 1:  Generate reasoning\n",
    "        # sample a minibatch of questions\n",
    "        # policy roll‚Äëout (actions + log‚Äëprobs)\n",
    "    # STEP 2:  Score the trajectory\n",
    "        # ORM: scalar reward for the final answer / PRM: scalar reward for the thought process\n",
    "    # STEP 3:  Reinforce the policy (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a81a6",
   "metadata": {},
   "source": [
    "---  \n",
    "# 4‚Äë A Deep Research Agent\n",
    "\n",
    "A deep-research agent pairs a reasoning model (e.g., deepseek-r1) with external tools for web search and retrieval. We will follow the ReAct pattern: the model writes short thoughts, decides when to call tools, reads observations, and continues reasoning until it can answer or reaches a step limit.\n",
    "\n",
    "We now combine a **search tool** with a reasoning model (e.g., `deepseek-r1`) in a multi-step setup. We follow the *ReAct* pattern (reason ‚Üí tool ‚Üí observation):\n",
    "\n",
    "1. The model reasoins and decides to use tools\n",
    "2. The agent searches and feed condensed snippets back as context\n",
    "3. Iterate until the model answers or hits a step limit\n",
    "\n",
    "We use `AgentType.OPENAI_FUNCTIONS`, which hides the loop inside the LangChain agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddgs import DDGS\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def ddg_search(query: str, k: int = 5) -> str:\n",
    "    # Use DDGS to run a simple web search and return joined snippets.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~6 lines of code)\n",
    "    \"\"\"\n",
    "\n",
    "search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=ddg_search,\n",
    "    description=\"Search the public web. Input: a plain English query. Returns: concatenated snippets.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "MODEL = \"deepseek-r1:8b\"\n",
    "question = \"What are the best resources to learn machine learning in 2025?\"\n",
    "\n",
    "# Step 1: Initialize the reasoning model via ChatOllama\n",
    "\"\"\"\n",
    "YOUR CODE HERE (1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Build the agent with tool access (DuckDuckGo Search) and function-calling interface (initialize_agent)\n",
    "\"\"\"\n",
    "YOUR CODE HERE (1 line of code)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Step 3: Ask a query and let the agent search + reason to produce an answer\n",
    "\"\"\"\n",
    "YOUR CODE HERE (2 lines of code)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1c3f7",
   "metadata": {},
   "source": [
    "# Optional (Multi-agent Deep Research)\n",
    "Instead of a single multi-step agent, you can design multiple collaborating agents such as a Planner, Searcher, Summarizer, and Verifier that pass information and refine each other‚Äôs outputs. This setup improves robustness, diversity of reasoning, and division of labor.\n",
    "\n",
    "Try building a simple setup with 2‚Äì3 agents that share goals and messages, for example Planner ‚Üí Researcher ‚Üí Writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59abf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_research(query, n=3):\n",
    "    # Run n independent research runs in parallel and return their answers.\n",
    "    # Steps: use ThreadPoolExecutor; submit n calls to your agent/search pipeline; gather results in order.\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE\n",
    "    \"\"\"\n",
    "\n",
    "answers = parallel_research(\"What are the best resources to learn ML in 2025?\")\n",
    "for i,a in enumerate(answers,1):\n",
    "    print(f\"[Run {i}] {a[:200]}‚Ä¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507d0a4",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "* Practised various inference‚Äëtime reasoning methods\n",
    "* Gained intuition about training reasoning models\n",
    "* You have built a **deep-research agent**: reasoning model like deep-seek r1 + ReAct-style agent + tool use (web search)\n",
    "* Try adding more tools, and extending the deep-research to a multi-agent system: many agents researching web in parallel.\n",
    "\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_research",
   "language": "python",
   "name": "deep_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
