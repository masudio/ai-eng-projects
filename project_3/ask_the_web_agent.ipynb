{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f7fe8ac",
   "metadata": {},
   "source": [
    "# Project 3: **Ask‚Äëthe‚ÄëWeb Agent**\n",
    "\n",
    "Welcome to Project‚ÄØ3! In this project, you will learn how to use tool‚Äëcalling LLMs, extend them with custom tools, and build a simplified *Perplexity‚Äëstyle* agent that answers questions by searching the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4311a6",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* Understand why tool calling is useful and how LLMs can invoke external tools.\n",
    "* Implement a minimal loop that parses the LLM's output and executes a Python function.\n",
    "* See how *function schemas* (docstrings and type hints) let us scale to many tools.\n",
    "* Use **LangChain** to get function‚Äëcalling capability for free (ReAct reasoning, memory, multi‚Äëstep planning).\n",
    "* Combine LLM with a web‚Äësearch tool to build a simple ask‚Äëthe‚Äëweb agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f16864",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "1. Environment setup\n",
    "2. Write simple tools and connect them to an LLM\n",
    "3. Standardize tool calling by writing `to_schema`\n",
    "4. Use LangChain to augment an LLM with your tools\n",
    "5. Build a Perplexity‚Äëstyle web‚Äësearch agent\n",
    "6. (Optional) A minimal backend and frontend UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# 1- Environment setup\n",
    "\n",
    "## 1.1- Conda environment\n",
    "\n",
    "Before we start coding, you need a reproducible setup. Open a terminal in the same directory as this notebook and run:\n",
    "\n",
    "```bash\n",
    "# Create and activate the conda environment\n",
    "conda env create -f environment.yml && conda activate web_agent\n",
    "\n",
    "# Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=web_agent --display-name \"web_agent\"\n",
    "```\n",
    "Once this is done, you can select ‚Äúweb_agent‚Äù from the Kernel ‚Üí Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "\n",
    "> Behind the scenes:\n",
    "> * Conda reads `environment.yml`, resolves the pinned dependencies, creates an isolated environment named `web_agent`, and activates it.\n",
    "> * `ollama pull` downloads the model so you can run it locally without API calls.\n",
    "\n",
    "\n",
    "## 1.2 Ollama setup\n",
    "\n",
    "In this project, we start with `gemma3-1B` because it is lightweight and runs on most machines. You can try other smaller or larger LLMs such as `mistral:7b`, `phi3:mini`, or `llama3.2:1b` to compare performance. Explore available models here: https://ollama.com/library\n",
    "\n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "```\n",
    "\n",
    "`ollama pull` downloads the model so you can run it locally without API calls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27158f",
   "metadata": {},
   "source": [
    "## 2- Tool¬†Calling\n",
    "\n",
    "LLMs are strong at answering questions, but they cannot directly access external data such as live web results, APIs, or computations. In real applications, agents rarely rely only on their internal knowledge. They need to query APIs, retrieve data, or perform calculations to stay accurate and useful. Tool calling bridges this gap by allowing the LLM to request actions from the outside world.\n",
    "\n",
    "\n",
    "We describe each tool‚Äôs interface in the model‚Äôs prompt, defining what it does and what arguments it expects. When the model decides that a tool is needed, it emits a structured output like: `TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Francisco\"}}`. Your code will detect this output, execute the corresponding function, and feed the result back to the LLM so the conversation continues.\n",
    "\n",
    "In this section, you will implement a simple `get_current_weather` function and teach the `gemma3` model how to use it when required in four steps:\n",
    "1. Implement the tool\n",
    "2. Create the instructions for the LLM\n",
    "3. Call the LLM with the prompt\n",
    "4. Parse the LLM output and call the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b5eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a536f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Implement the tool\n",
    "# ---------------------------------------------------------\n",
    "# Your goal: give the model a way to access weather information.\n",
    "# You can either:\n",
    "#   (a) Call a real weather API (for example, OpenWeatherMap), or\n",
    "#   (b) Create a dummy function that returns a fixed response (e.g., \"It is 23¬∞C and sunny in San Francisco.\")\n",
    "#\n",
    "# Requirements:\n",
    "#   ‚Ä¢ The function should be named `get_current_weather`\n",
    "#   ‚Ä¢ It should take two arguments:\n",
    "#         - city: str\n",
    "#         - unit: str = \"celsius\"\n",
    "#   ‚Ä¢ Return a short, human-readable sentence describing the weather.\n",
    "#\n",
    "# Example expected behavior:\n",
    "#   get_current_weather(\"San Francisco\") ‚Üí \"It is 23¬∞C and sunny in San Francisco.\"\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-3 lines of code)\n",
    "\"\"\"\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_current_weather(city: str, unit: str = \"celsius\"):\n",
    "    \"\"\"Get the current weather in a given city\"\"\"\n",
    "    API_KEY = '2ea40921be0a7ca1ae28b94015d71a2e'\n",
    "\n",
    "    # OpenWeatherMap uses 'metric' for Celsius, 'imperial' for Fahrenheit\n",
    "    units = \"metric\" if unit == \"celsius\" else \"imperial\"\n",
    "    unit_symbol = \"¬∞C\" if unit == \"celsius\" else \"¬∞F\"\n",
    "    \n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units={units}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            temp = data[\"main\"][\"temp\"]\n",
    "            description = data[\"weather\"][0][\"description\"]\n",
    "            return f\"It is {temp}{unit_symbol} and {description} in {city}.\"\n",
    "        else:\n",
    "            return f\"Could not fetch weather for {city}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching weather: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a43c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Create the prompt for the LLM to call tools\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Build the system and user prompts that instruct the model when and how\n",
    "#   to use your tool (`get_current_weather`).\n",
    "#\n",
    "# What to include:\n",
    "#   ‚Ä¢ A SYSTEM_PROMPT that tells the model about the tool use and describe the tool\n",
    "#   ‚Ä¢ A USER_QUESTION with a user query that should trigger the tool.\n",
    "#       Example: \"What is the weather in San Diego today?\"\n",
    "\n",
    "# Try experimenting with different system and user prompts\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can answer questions and help with tasks.\n",
    "You have access to a tool called \"get_current_weather\" that can give you the current weather in a given city.\n",
    "To use it, you can call the tool with the city name and unit as arguments, like this:\n",
    "\n",
    "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\", \"unit\": \"celsius\"}}\n",
    "\n",
    "Here is the user question:\n",
    "\"\"\"\n",
    "USER_QUESTION = \"What is the weather in Calgary today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebb062",
   "metadata": {},
   "source": [
    "Now that you have defined a tool and shown the model how to use it, the next step is to call the LLM using your prompt.\n",
    "\n",
    "Start the **Ollama** server in a terminal with `ollama serve`. This launches a local API endpoint that listens for LLM requests. Once the server is running, return to the notebook and in the next cell send a query to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "027cb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find out the current weather in Calgary, I can use the tool \"get_current_weather\". Here's how I will call it:\n",
      "\n",
      "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"Calgary\", \"unit\": \"celsius\"}}\n",
      "\n",
      "Once the tool returns the data, I can tell you the weather in Calgary today.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 3: Call the LLM with your prompt\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Send SYSTEM_PROMPT + USER_QUESTION to the model.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use the Ollama client to create a chat completion. \n",
    "#       - You may find some examples here: https://platform.openai.com/docs/guides/text\n",
    "#       - If you are unsure, search the web for \"client.chat.completions.create\"\n",
    "#   2. Print the raw response.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should return something like:\n",
    "#   TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"San Diego\"}}\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5-10 lines of code)\n",
    "\"\"\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"mistral:7b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94aeb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"get_current_weather\", \"args\": {\"city\": \"Calgary\", \"unit\": \"celsius\"}}\n",
      "Calling tool `get_current_weather` with args {'city': 'Calgary', 'unit': 'celsius'}\n",
      "It is 7.74¬∞C and broken clouds in Calgary.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4: Parse the LLM output and call the tool\n",
    "# ---------------------------------------------------------\n",
    "# Task:\n",
    "#   Detect when the model requests a tool, extract its name and arguments,\n",
    "#   and execute the corresponding function.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Search for the text pattern \"TOOL_CALL:{...}\" in the model output.\n",
    "#   2. Parse the JSON inside it to get the tool name and args.\n",
    "#   3. Call the matching function (e.g., get_current_weather).\n",
    "#\n",
    "# Expected:\n",
    "#   You should see a line like:\n",
    "#       Calling tool `get_current_weather` with args {'city': 'San Diego'}\n",
    "#       Result: It is 23¬∞C and sunny in San Diego.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import re, json\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5-10 lines of code)\n",
    "\"\"\"\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_tool_call_json(text):\n",
    "    # Find the start of the JSON\n",
    "    match = re.search(r'TOOL_CALL:\\s*(\\{)', text)\n",
    "    if not match:\n",
    "        return None\n",
    "    \n",
    "    start_pos = match.start(1)\n",
    "    brace_count = 0\n",
    "    in_string = False\n",
    "    escape_next = False\n",
    "    \n",
    "    for i, char in enumerate(text[start_pos:], start=start_pos):\n",
    "        if escape_next:\n",
    "            escape_next = False\n",
    "            continue\n",
    "        \n",
    "        if char == '\\\\':\n",
    "            escape_next = True\n",
    "            continue\n",
    "            \n",
    "        if char == '\"':\n",
    "            in_string = not in_string\n",
    "        elif not in_string:\n",
    "            if char == '{':\n",
    "                brace_count += 1\n",
    "            elif char == '}':\n",
    "                brace_count -= 1\n",
    "                if brace_count == 0:\n",
    "                    return text[start_pos:i+1]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "TOOL_MAP = {\n",
    "    \"get_current_weather\": get_current_weather\n",
    "}\n",
    "\n",
    "response_string = response.choices[0].message.content\n",
    "\n",
    "# Usage:\n",
    "tool_call_json = extract_tool_call_json(response_string)\n",
    "print(tool_call_json)\n",
    "\n",
    "if tool_call_json:\n",
    "    tool_call = json.loads(tool_call_json)\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    tool_args = tool_call[\"args\"]\n",
    "    print(f\"Calling tool `{tool_name}` with args {tool_args}\")\n",
    "    result = TOOL_MAP[tool_name](**tool_args)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be26661",
   "metadata": {},
   "source": [
    "# 3- Standadize tool calling\n",
    "\n",
    "So far, we handled tool calling manually by writing one regex and one hard-coded function. This approach does not scale if we want to add more tools. Adding more tools would mean more `if/else` blocks and manual edits to the `TOOL_SPEC` prompt.\n",
    "\n",
    "To make the system flexible, we can standardize tool definitions by automatically reading each function‚Äôs signature, converting it to a JSON schema, and passing that schema to the LLM. This way, the LLM can dynamically understand which tools exist and how to call them without requiring manual updates to prompts or conditional logic.\n",
    "\n",
    "Next, you will implement a small helper that extracts metadata from functions and builds a schema for each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce911b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': [{'default': <class 'inspect._empty'>,\n",
      "            'name': 'city',\n",
      "            'type': \"<class 'str'>\"},\n",
      "           {'default': 'celsius', 'name': 'unit', 'type': \"<class 'str'>\"}],\n",
      "  'tool_name': 'get_current_weather'},\n",
      " {'args': [{'default': 8,\n",
      "            'name': 'length',\n",
      "            'type': \"<class 'inspect._empty'>\"}],\n",
      "  'tool_name': 'print_random_string'}]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Generate a JSON schema for a tool automatically\n",
    "# ---------------------------------------------------------\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use `inspect.signature` to get function parameters.\n",
    "#   2. For each argument, record its name, type, and description.\n",
    "#   3. Build a schema containing:\n",
    "#   4. Test your helper on `get_current_weather` and print the result.\n",
    "#\n",
    "# Expected:\n",
    "#   A dictionary describing the tool (its name, args, and types).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pprint import pprint\n",
    "import inspect\n",
    "\n",
    "\n",
    "def to_schema(fn):\n",
    "    \"\"\"\n",
    "    YOUR CODE HERE (~8-15 lines of code)\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(fn)\n",
    "    json_dict = {\"tool_name\": fn.__name__, \"args\": []}\n",
    "    for param in sig.parameters.values():\n",
    "        json_dict[\"args\"].append({\n",
    "            \"name\": param.name,\n",
    "            \"type\": str(param.annotation),\n",
    "            \"default\": param.default\n",
    "        })\n",
    "    return json_dict\n",
    "\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "def print_random_string(length=8):\n",
    "    rand_str = ''.join(random.choices(string.ascii_uppercase, k=length))\n",
    "    print(rand_str)\n",
    "\n",
    "\n",
    "functions = [\n",
    "    get_current_weather,\n",
    "    print_random_string,\n",
    "]\n",
    "\n",
    "full_tool_schema = []\n",
    "for fn in functions:\n",
    "    full_tool_schema.append(to_schema(fn))\n",
    "pprint(full_tool_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1163f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"tool_name\": \"get_current_weather\", \"args\": [{\"name\": \"city\", \"type\": \"<class 'str'>\", \"default\": \"<class 'inspect._empty'>\"}, {\"name\": \"unit\", \"type\": \"<class 'str'>\", \"default\": \"celsius\"}]}, {\"tool_name\": \"print_random_string\", \"args\": [{\"name\": \"length\", \"type\": \"<class 'inspect._empty'>\", \"default\": 8}]}]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Provide the tool schema to the model\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Give the model a \"menu\" of available tools so it can choose\n",
    "#   which one to call based on the user‚Äôs question.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Add an extra system message (e.g., name=\"tool_spec\")\n",
    "#      containing the JSON schema(s) of your tools.\n",
    "#   2. Include SYSTEM_PROMPT and the user question as before.\n",
    "#   3. Send the messages to the model (e.g., gemma3:1b).\n",
    "#   4. Print the raw model output to see if it picks the right tool.\n",
    "#\n",
    "# Expected:\n",
    "#   The model should produce a structured TOOL_CALL indicating\n",
    "#   which tool to use and with what arguments.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import json\n",
    "tool_spec = json.dumps(full_tool_schema, default=str)\n",
    "print(tool_spec)\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5-12 lines of code)\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT_2 = \"\"\"\n",
    "You are a helpful assistant that can answer questions and help with tasks.\n",
    "You have access to a list of tools that you can use to answer the user's question.\n",
    "To use a tool, you can call the tool by providing the tool name and the arguments, like this:\n",
    "\n",
    "TOOL_CALL: {{\"name\": \"get_current_weather\", \"args\": {{\"city\": \"San Diego\", \"unit\": \"celsius\"}}}}\n",
    "\n",
    "Here is the list of tools available to you:\n",
    "{tool_spec}\n",
    "\n",
    "Here is the user question:\n",
    "\"\"\".format(tool_spec=tool_spec)\n",
    "\n",
    "USER_QUESTION = \"What is the weather in Calgary today?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ecb2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To answer the user's question, I will call the `get_current_weather` tool with the argument `city` set to 'Calgary' and default unit as 'Celsius'. The response will contain information about the current weather in Calgary.\n",
      "\n",
      "TOOL_CALL: {\"name\": \"get_current_weather\", \"args\": {\"city\": \"Calgary\", \"unit\": \"celsius\"}}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"mistral:7b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_2},\n",
    "        {\"role\": \"user\", \"content\": USER_QUESTION}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ec86e",
   "metadata": {},
   "source": [
    "## 4-‚ÄØLangChain for Tool Calling\n",
    "So far, you built a simple tool-calling pipeline manually. While this helps you understand the logic, it does not scale well when working with multiple tools, complex parsing, or multi-step reasoning.\n",
    "\n",
    "LangChain simplifies this process. You only need to declare your tools, and its *Agent* abstraction handles when to call a tool, how to use it, and how to continue reasoning afterward.\n",
    "\n",
    "In this section, you will use the **ReAct** Agent (Reasoning + Acting). It alternates between reasoning steps and tool use, producing clearer and more reliable results. We will explore reasoning-focused models in more depth next week.\n",
    "\n",
    "The following links might be helpful:\n",
    "- https://python.langchain.com/api_reference/langchain/agents/langchain.agents.initialize.initialize_agent.html\n",
    "- https://python.langchain.com/docs/integrations/tools/\n",
    "- https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.llms.LLM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c609d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Define tools for LangChain\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Convert your weather function into a LangChain-compatible tool.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import `tool` from `langchain.tools`.\n",
    "#   2. Keep your existing `get_current_weather` helper as before.\n",
    "#   3. Create a new function (e.g., get_weather) that calls it.\n",
    "#   4. Add the `@tool` decorator so LangChain can register it automatically.\n",
    "#\n",
    "# Notes:\n",
    "#   ‚Ä¢ The decorator converts your Python function into a standardized tool object.\n",
    "#   ‚Ä¢ Start with keeping the logic simple and offline-friendly.\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\"\n",
    "@tool\n",
    "def get_weather(city: str, unit: str = \"celsius\"):\n",
    "    \"\"\"Gets the weather of the given city in the given units\"\"\"\n",
    "    return get_current_weather(city, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9552348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_weather\",\n",
      "  \"action_input\": {\n",
      "    \"city\": \"San Diego\",\n",
      "    \"unit\": \"celsius\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt is 20.22¬∞C and broken clouds in San Diego.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Based on the weather report, it seems like there's a chance of some cloud cover but no heavy rain forecasted for today in San Diego. However, it's always good to be prepared. I would recommend carrying an umbrella just in case, but you might not need to use it extensively.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Given the weather forecast, it's recommended to carry an umbrella as there's a chance of cloud cover, but heavy rain is unlikely.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"I'm in San Diego - should I bring an umbrella today?\",\n",
       " 'output': \"Given the weather forecast, it's recommended to carry an umbrella as there's a chance of cloud cover, but heavy rain is unlikely.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the LangChain Agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Connect your tool to a local LLM using LangChain‚Äôs ReAct-style agent.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import the required classes:\n",
    "#        - ChatOllama (for local model access)\n",
    "#        - initialize_agent, Tool, AgentType\n",
    "#   2. Create an LLM instance (e.g., model=\"gemma3:1b\", temperature=0).\n",
    "#   3. Add your tool(s) to a list\n",
    "#   4. Initialize the agent using initialize_agent\n",
    "#   5. Test the agent with a natural question (e.g., \"Do I need an umbrella in Seattle today?\").\n",
    "#\n",
    "# Expected:\n",
    "#   The model should reason through the question, call your tool,\n",
    "#   and produce a final answer in plain language.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\"\n",
    "llm = ChatOllama(model=\"mistral:7b\", temperature=0)\n",
    "tools = [get_weather]\n",
    "\n",
    "agent_executor = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent_executor.invoke(\"I'm in San Diego - should I bring an umbrella today?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9e8fb",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "The console log displays the **Thought‚ÄØ‚Üí‚ÄØAction‚ÄØ‚Üí‚ÄØObservation‚ÄØ‚Üí‚ÄØ‚Ä¶** loop until the agent produces its final answer. Because `verbose=True`, LangChain prints each intermediate reasoning step.\n",
    "\n",
    "If you want to add more tools, simply append them to the tools list. LangChain will handle argument validation, schema generation, and tool-calling logic automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5- Perplexity‚ÄëStyle Web Search\n",
    "Agents become much more powerful when they can look up real information on the web instead of relying only on their internal knowledge.\n",
    "\n",
    "In this section, you will combine everything you have learned to build a simple Ask-the-Web Agent. You will integrate a web search tool (DuckDuckGo) and make it available to the agent using the same tool-calling approach as before.\n",
    "\n",
    "This will let the model retrieve fresh results, reason over them, and generate an informed answer‚Äîsimilar to how Perplexity works.\n",
    "\n",
    "You may find some examples from the following links:\n",
    "- https://pypi.org/project/duckduckgo-search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v4/rz21fv5s5fn_4nv27jvlbh3m0000gn/T/ipykernel_35891/1810273572.py:28: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = search_web(\"python programming\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "[{'title': 'Welcome to Python.org', 'href': 'https://www.python.org/', 'body': 'Python is a versatile and easy-to-learn programming language that lets you work quickly and integrate systems more effectively. Learn Python basics, download the latest version, access documentation, find jobs, events, success stories and more on the official website.'}, {'title': 'Python Tutorial - W3SchoolsUsage example', 'href': 'https://www.w3schools.com/python/', 'body': 'W3Schools offers a comprehensive and interactive Python tutorial with examples, exercises, quizzes, and references. Learn how to create web applications, handle files and databases, and get certified by completing the PYTHON course. See more on w3schools'}, {'title': 'Learn Python Programming', 'href': 'https://www.programiz.com/python-programming', 'body': 'A comprehensive guide to learn Python, one of the top programming languages in the world, widely used in AI, data science, and web development. Find free tutorials, interactive courses, online compiler, and career tips for beginners and advanced learners.'}, {'title': 'Learn Python Programming Language - GeeksforGeeks', 'href': 'https://www.geeksforgeeks.org/python/python-programming-language-tutorial/', 'body': 'Oct 13, 2025 ¬∑ In this section, we‚Äôll cover the basics of Python programming, including installing Python, writing first program, understanding comments and working with variables, keywords and operators.'}, {'title': 'How to Use Python: Your First Steps ‚Äì Real Python', 'href': 'https://realpython.com/python-first-steps/', 'body': \"Oct 13, 2025 ¬∑ Learn the basics of Python syntax, installation, error handling, and code style in this tutorial. You'll also create your first Python program and test your knowledge with a quiz.\"}]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 1: Add a web search tool\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Create a tool that lets the agent search the web and return results.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Use DuckDuckGo for quick, open web searches.\n",
    "#   2. Write a helper function (e.g., search_web) that:\n",
    "#        ‚Ä¢ Takes a query string\n",
    "#        ‚Ä¢ Uses DDGS to fetch top results (titles + URLs)\n",
    "#        ‚Ä¢ Returns them as a formatted string\n",
    "#   3. Wrap it with the @tool decorator to make it available to LangChain.\n",
    "\n",
    "\n",
    "from ddgs import DDGS\n",
    "from langchain.tools import tool\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5-10 lines of code)\n",
    "\"\"\"\n",
    "@tool\n",
    "def search_web(query: str):\n",
    "    \"\"\"Searches the web for the given query\"\"\"\n",
    "    results = DDGS().text(query, max_results=5)\n",
    "    return results\n",
    "\n",
    "results = search_web(\"python programming\")\n",
    "print(\"Results:\")\n",
    "print(results)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Initialize the web-search agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Connect your `web_search` tool to a language model\n",
    "#   so the agent can search and reason over real data.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Import `initialize_agent` and `AgentType`.\n",
    "#   2. Create an LLM (e.g., ChatOllama).\n",
    "#   3. Add your `web_search` tool to the tools list.\n",
    "#   4. Initialize the agent using: initialize_agent\n",
    "#   5. Keep `verbose=True` to observe reasoning steps.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should be ready to accept user queries\n",
    "#   and use your web search tool when needed.\n",
    "# ---------------------------------------------------------\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~5 lines of code)\n",
    "\"\"\"\n",
    "llm = ChatOllama(model=\"mistral:7b\", temperature=0)\n",
    "tools_list = [get_weather, search_web]\n",
    "agent_executor = initialize_agent(\n",
    "    tools_list,\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let‚Äôs see the agent's output in action with a real example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_weather\",\n",
      "  \"action_input\": {\n",
      "    \"city\": \"New York\",\n",
      "    \"unit\": \"celsius\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt is 17.13¬∞C and few clouds in New York.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I need to find out if the weather in New York today will affect the stock market. To do this, I'll search the web for information on how weather affects the stock market, specifically focusing on New York.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"search_web\",\n",
      "  \"action_input\": {\n",
      "    \"query\": \"how does weather in New York affect the stock market\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m[{'title': 'Data Study on Weather vs. theStockMarket', 'href': 'https://nycdatascience.com/blog/student-works/data-study-on-weather-vs-the-stock-market/', 'body': 'Doesweatheraffectthe financial markets? Is there a correlation between the two? Dostock prices and equities trading transaction volumes follow a pattern in relation to different weather conditions?'}, {'title': 'DoesWeatherAffecttheStockMarket?', 'href': 'https://www.linkedin.com/pulse/does-weather-affect-stock-market-bernard-ong', 'body': \"Weather and stocks. Doesweatheraffectthe financial markets? Is there a correlation between the two?It depends on how one did the analysis. Some weather patterns doaffectthestockmarket, but some don't.\"}, {'title': 'Does the weatheraffectstockmarket volatility?', 'href': 'https://ideas.repec.org/p/pra/mprapa/34128.html', 'body': '\"DoesWeather Still AffectTheStockMarket?,\" Schmalenbach Journal of Business Research, Springer, vol. 74(1), pages 1-35, March. Apergis, Nicholas & Gupta, Rangan, 2017. \"Can (unusual) weather conditions inNewYork predict South African stock returns...'}, {'title': 'Why good weather isn‚Äôt a good thing for stockmarkets | University of...', 'href': 'https://www.port.ac.uk/news-events-and-blogs/news/why-good-weather-isnt-a-good-thing-for-stock-markets', 'body': 'A study suggests sunny weather can influence how investors perform and affectstockmarket prices.Sunshine levels have a significant impact on the bidding behaviour of stockmarket investors, according to new research.'}, {'title': 'DoesWeatherAffectTheStockMarket? | teuscherfifthavenue', 'href': 'https://teuscherfifthavenue.com/does-weather-affect-the-stock-market/', 'body': 'More than 636,000 market observations by 5,456 brokerage-houses were observed. Since the researchers knew where the analysts were located, it was easy to link the weather to publications.'}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m There are several articles suggesting that weather can affect the stock market, but the specific impact on New York's stock market today is not explicitly stated in these sources. To get a more precise answer, I would need to analyze the data from each article and find the relevant information about New York. However, as a responsible AI, I won't spend too much time on this task since it requires human-level understanding and interpretation of complex financial data.\n",
      "\n",
      "Instead, I will provide you with the links to these articles so that you can further investigate if you wish:\n",
      "1. Data Study on Weather vs. the Stock Market - <https://nycdatascience.com/blog/student-works/data-study-on-weather-vs-the-stock-market/>\n",
      "2. Does weather affect the stock market? - <https://www.linkedin.com/pulse/does-weather-affect-stock-market-bernard-ong>\n",
      "3. Does the weather affect stock market volatility? - <https://ideas.repec.org/p/pra/mprapa/34128.html>\n",
      "4. Why good weather isn‚Äôt a good thing for stock markets | University of Portsmouth - <https://www.port.ac.uk/news-events-and-blogs/news/why-good-weather-isnt-a-good-thing-for-stock-markets>\n",
      "5. Does Weather Affect The Stock Market? | teuscherfifthavenue - <https://teuscherfifthavenue.com/does-weather-affect-the-stock-market/>\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"I have provided you with links to articles that discuss the potential impact of weather on the stock market, specifically focusing on New York. You can further investigate these sources if you wish.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the weather in New York today and how will it affect the stock market?',\n",
       " 'output': 'I have provided you with links to articles that discuss the potential impact of weather on the stock market, specifically focusing on New York. You can further investigate these sources if you wish.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Test your Ask-the-Web agent\n",
    "# ---------------------------------------------------------\n",
    "# Goal:\n",
    "#   Verify that the agent can search the web and return\n",
    "#   a summarized answer based on real results.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Ask a natural question that requires live information,\n",
    "#      for example: \"What are the current events in San Francisco this week?\"\n",
    "#   2. Call agent.\n",
    "#\n",
    "# Expected:\n",
    "#   The agent should call `web_search`, retrieve results,\n",
    "#   and generate a short summary response.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2-5 lines of code)\n",
    "\"\"\"\n",
    "\n",
    "agent_executor.invoke(\"What is the weather in New York today and how will it affect the stock market?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6- A minimal UI\n",
    "This project includes a simple **React** front end that sends the user‚Äôs question to a FastAPI back end and streams the agent‚Äôs response in real time. To run the UI:\n",
    "\n",
    "1- Open a terminal and start the Ollama server: `ollama serve`.\n",
    "\n",
    "2- In a second terminal, navigate to the frontend folder and install dependencies:`npm install`.\n",
    "\n",
    "3- In the same terminal, start the FastAPI back‚Äëend: `uvicorn app:app --reload --port 8000`\n",
    "\n",
    "4- Open a third terminal, stay in the frontend folder, and start the React dev server: `npm run dev`\n",
    "\n",
    "5- Visit `http://localhost:5173/` in your browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "* You have built a **web‚Äëenabled agent**: tool calling ‚Üí JSON schema ‚Üí LangChain ReAct ‚Üí web search ‚Üí simple UI.\n",
    "* Try adding more tools, such as news or finance APIs.\n",
    "* Experiment with multiple tools, different models, and measure accuracy vs. hallucination.\n",
    "\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_agent",
   "language": "python",
   "name": "web_agent"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
