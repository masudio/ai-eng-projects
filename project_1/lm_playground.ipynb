{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fe523821",
      "metadata": {
        "id": "fe523821"
      },
      "source": [
        "\n",
        "# Projectâ€¯1: Build an LLM Playground\n",
        "\n",
        "Welcome! In this project, youâ€™ll learn foundations of large language models (LLMs). Weâ€™ll keep the code minimal and the explanations highâ€‘level so that anyone who can run a Python cell can follow along.  \n",
        "\n",
        "We'll be using Google Colab for this project. Colab is a free, browser-based platform that lets you run Python code and machine learning models without installing anything on your local computer. Click the button below to open this notebook directly in Google Colab and get started!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb8584e",
      "metadata": {
        "id": "fdb8584e"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bytebyteai/ai-eng-projects/blob/main/project_1/lm_playground.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08e82492",
      "metadata": {
        "id": "08e82492"
      },
      "source": [
        "---\n",
        "## Learning Objectives  \n",
        "* **Tokenization** and how raw text is tokenized into a sequene of discrete tokens\n",
        "* Inspect **GPT2** and **Transformer architecture**\n",
        "* Loading pre-trained LLMs using **Hugging Face**\n",
        "* **Decoding strategies** to generate text from LLMs\n",
        "* Completion versus **intrusction fine-tuned** LLMs\n",
        "\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1235110e",
      "metadata": {
        "id": "1235110e"
      },
      "outputs": [],
      "source": [
        "import torch, transformers, tiktoken\n",
        "print(\"torch\", torch.__version__, \"| transformers\", transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c1eb0b",
      "metadata": {
        "id": "d4c1eb0b"
      },
      "source": [
        "# 1 - Tokenization\n",
        "\n",
        "A neural network canâ€™t digest raw text. They need **numbers**. Tokenization is the process of converting text into IDs. In this section, you'll learn how tokenization is implemented in practice.\n",
        "\n",
        "Tokenization methods generally fall into three categories:\n",
        "1. Word-level\n",
        "2. Character-level\n",
        "3. Subword-level"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d234dc0",
      "metadata": {
        "id": "1d234dc0"
      },
      "source": [
        "### 1.1 - Wordâ€‘level tokenization\n",
        "\n",
        "Split text on whitespace and store each **word** as a token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d784a288",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d784a288",
        "outputId": "1a7b2b84-d92f-4709-db2f-75df2c3c1d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 22 words\n",
            "First 15 vocab entries: ['[PAD]', '[UNK]', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'Tokenization', 'converts', 'text', 'to']\n",
            "\n",
            "Input text : The brown unicorn jumps\n",
            "Token IDs  : [2, 4, 1, 6]\n",
            "Decoded    : The brown [UNK] jumps\n"
          ]
        }
      ],
      "source": [
        "# 1. Tiny corpus\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"Tokenization converts text to numbers\",\n",
        "    \"Large language models predict the next token\"\n",
        "]\n",
        "\n",
        "# 2. Build the vocabulary\n",
        "PAD, UNK = \"[PAD]\", \"[UNK]\"\n",
        "vocab = [PAD, UNK]\n",
        "word2id = {PAD: 0, UNK: 1}\n",
        "id2word = {0: PAD, 1: UNK}\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "current_id = 2\n",
        "for line in corpus:\n",
        "  for word in line.split(\" \"):\n",
        "    if word not in vocab:\n",
        "      vocab.append(word)\n",
        "      word2id[word] = current_id\n",
        "      id2word[current_id] = word\n",
        "      current_id += 1\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)} words\")\n",
        "print(\"First 15 vocab entries:\", vocab[:15])\n",
        "\n",
        "# 3. Encode / decode\n",
        "def encode(text):\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for word in text.split(\" \"):\n",
        "      if word in word2id:\n",
        "        id = word2id[word]\n",
        "      else:\n",
        "        id = word2id[UNK]\n",
        "      result.append(id)\n",
        "\n",
        "    return result\n",
        "\n",
        "def decode(ids):\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for id in ids:\n",
        "      if id in id2word:\n",
        "        word = id2word[id]\n",
        "      else:\n",
        "        word = UNK\n",
        "      result.append(word)\n",
        "\n",
        "    return \" \".join(result)\n",
        "\n",
        "# 4. Demo\n",
        "sample = \"The brown unicorn jumps\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0edab2c2",
      "metadata": {
        "id": "0edab2c2"
      },
      "source": [
        "Word-level tokenization has two major limitations:\n",
        "1. Large vocabulary size\n",
        "2. Out-of-vocabulary (OOV) issue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a379bac7",
      "metadata": {
        "id": "a379bac7"
      },
      "source": [
        "### 1.2 - Characterâ€‘level tokenization\n",
        "\n",
        "Every single character (including spaces and emojis) gets its own ID. This guarantees zero outâ€‘ofâ€‘vocabulary issues but very long sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4ac29144",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ac29144",
        "outputId": "1389f387-aee6-4473-98de-68c5ddb4f231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 54 (52 letters + 2 specials)\n",
            "\n",
            "Input text : Hello\n",
            "Token IDs  : [17, 10, 24, 24, 30]\n",
            "Decoded    : Hello\n"
          ]
        }
      ],
      "source": [
        "# 1. Build a fixed vocabulary # aâ€“z + Aâ€“Z + padding + unkwown\n",
        "import string\n",
        "\n",
        "vocab = []\n",
        "char2id = {}\n",
        "id2char = {}\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "vocab.append(\"[PAD]\")\n",
        "vocab.append(\"[UNK]\")\n",
        "for char in string.ascii_lowercase:\n",
        "  vocab.append(char)\n",
        "  char2id[char] = len(vocab) - 1\n",
        "  id2char[len(vocab) - 1] = char\n",
        "  vocab.append(char.upper())\n",
        "  char2id[char.upper()] = len(vocab) - 1\n",
        "  id2char[len(vocab) - 1] = char.upper()\n",
        "print(f\"Vocabulary size: {len(vocab)} (52 letters + 2 specials)\")\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for char in text:\n",
        "      if char in char2id:\n",
        "        id = char2id[char]\n",
        "      else:\n",
        "        id = char2id[UNK]\n",
        "      result.append(id)\n",
        "\n",
        "    return result\n",
        "\n",
        "def decode(ids):\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for id in ids:\n",
        "      if id in id2char:\n",
        "        char = id2char[id]\n",
        "      else:\n",
        "        char = UNK\n",
        "      result.append(char)\n",
        "\n",
        "    return \"\".join(result)\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Hello\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Decoded    :\", recovered)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391275bd",
      "metadata": {
        "id": "391275bd"
      },
      "source": [
        "### 1.3 - Subwordâ€‘level tokenization\n",
        "\n",
        "Sub-word methods such as `Byte-Pair Encoding (BPE)`, `WordPiece`, and `SentencePiece` **learn** the most common character and gorup them into new tokens. For example, the word `unbelievable` might turn into three tokens: `[\"un\", \"believ\", \"able\"]`. This approach strikes a balance between word-level and character-level methods and fix their limitations.\n",
        "\n",
        "For example, `BPE` algorithm forms the vocabulary using the following steps:\n",
        "1. **Start with bytes** â†’ every character is its own token.  \n",
        "2. **Count all adjacent pairs** in a huge corpus.  \n",
        "3. **Merge the most frequent pair** into a new token.  \n",
        "   *Repeat steps 2-3* until you hit the target vocab size (e.g., 50 k).\n",
        "\n",
        "Let's see `BPE` in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4675e67a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4675e67a",
        "outputId": "3d29d31f-ac0d-42d9-df2b-7f400b565562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input text : Unbelievable tokenization powers! ðŸš€\n",
            "Token IDs  : [3118, 6667, 11203, 540, 11241, 1634, 5635, 0, 12520, 248, 222]\n",
            "Tokens     : ['Un', 'bel', 'iev', 'able', 'Ä token', 'ization', 'Ä powers', '!', 'Ä Ã°Å', 'Ä¼', 'Ä¢']\n",
            "Decoded    : Unbelievable tokenization powers! ðŸš€\n"
          ]
        }
      ],
      "source": [
        "# 1. Load a pretrained BPE tokenizer (GPT-2 uses BPE).\n",
        "# Refer to  https://huggingface.co/docs/transformers/en/fast_tokenizers\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "bpe_tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 2. Encode / decode\n",
        "def encode(text):\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "    bpe_tokens = bpe_tok.tokenize(text)\n",
        "    ids = bpe_tok.convert_tokens_to_ids(bpe_tokens)\n",
        "    return ids\n",
        "\n",
        "\n",
        "def decode(ids):\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "    bpe_tokens = bpe_tok.convert_ids_to_tokens(ids)\n",
        "    text = bpe_tok.convert_tokens_to_string(bpe_tokens)\n",
        "    return text\n",
        "\n",
        "# 3. Demo\n",
        "sample = \"Unbelievable tokenization powers! ðŸš€\"\n",
        "ids = encode(sample)\n",
        "recovered = decode(ids)\n",
        "\n",
        "print(\"\\nInput text :\", sample)\n",
        "print(\"Token IDs  :\", ids)\n",
        "print(\"Tokens     :\", bpe_tok.convert_ids_to_tokens(ids))\n",
        "print(\"Decoded    :\", recovered)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badaa5a8",
      "metadata": {
        "id": "badaa5a8"
      },
      "source": [
        "### 1.4 - TikToken\n",
        "\n",
        "`tiktoken` is a production-ready library which offers highâ€‘speed tokenization used by OpenAI models.  \n",
        "Let's compare the older **gpt2** encoding with the newer **cl100k_base** used in GPTâ€‘4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "7704c470",
      "metadata": {
        "id": "7704c470"
      },
      "outputs": [],
      "source": [
        "# Use gpt2 and cl100k_base to encode and decode the following text\n",
        "# Refer to https://github.com/openai/tiktoken\n",
        "import tiktoken\n",
        "\n",
        "sentence = \"The ðŸŒŸ star-player scored 40 points!\"\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "gpt2_tok = tiktoken.get_encoding(\"gpt2\")\n",
        "cl100k_base_tok = tiktoken.get_encoding(\"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8c1023",
      "metadata": {
        "id": "5e8c1023"
      },
      "source": [
        "Experiment: try new sentences, emojis, code snippets, or other languages. If you are interested, try implementing the BPE algorithm yourself.\n",
        "\n",
        "### 1.5 - Key Takeaways\n",
        "\n",
        "* **Wordâ€‘level**: simple but brittle (OOV problems).  \n",
        "* **Characterâ€‘level**: robust but produces long sequences.  \n",
        "* **BPE / Byteâ€‘Level BPE**: middle ground used by most LLMs.  \n",
        "* **tiktoken**: shows how production models tokenize with preâ€‘trained subâ€‘word vocabularies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a758ba",
      "metadata": {
        "id": "c2a758ba"
      },
      "source": [
        "# 2. What is a Language Model?\n",
        "\n",
        "At its core, a **language model (LM)** is just a *very large* mathematical function built from many neural-network layers.  \n",
        "Given a sequence of tokens `[tâ‚, tâ‚‚, â€¦, tâ‚™]`, it learns to output a probability for the next token `tâ‚™â‚Šâ‚`.\n",
        "\n",
        "\n",
        "Each layer applies a simple operation (matrix multiplication, attention, etc.). Stacking hundreds of these layers lets the model capture patterns and statistical relations from text. The final output is a vector of scores that says, â€œhow likely is each possible token to come next?â€\n",
        "\n",
        "> Think of the whole network as **one gigantic equation** whose parameters were tuned during training to minimize prediction error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0c7399",
      "metadata": {
        "id": "8f0c7399"
      },
      "source": [
        "\n",
        "### 2.1 - A Single `Linear` Layer\n",
        "\n",
        "Before we explore Transformer, letâ€™s start tiny:\n",
        "\n",
        "* A **Linear layer** performs `y = Wx + b`  \n",
        "  * `x` â€“ input vector  \n",
        "  * `W` â€“ weight matrix (learned)  \n",
        "  * `b` â€“ bias vector (learned)\n",
        "\n",
        "Although this looks basic, chaining thousands of such linear transforms (with nonlinearities in between) gives neural nets their expressive power.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e425948a",
      "metadata": {
        "id": "e425948a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.weight.t() + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "13e5e225",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13e5e225",
        "outputId": "d8374272-5c44-45e3-f67f-28fba129d2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : tensor([ 1.0000, -1.0000,  0.5000])\n",
            "Weights: Parameter containing:\n",
            "tensor([[-0.1322,  0.2951,  0.3686],\n",
            "        [ 0.1851,  0.5705,  0.4866]], requires_grad=True)\n",
            "Bias   : Parameter containing:\n",
            "tensor([-0.0416, -0.2374], requires_grad=True)\n",
            "Output : tensor([-0.2846, -0.3795], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn, torch\n",
        "\n",
        "lin = nn.Linear(3, 2)\n",
        "x = torch.tensor([1.0, -1.0, 0.5])\n",
        "print(\"Input :\", x)\n",
        "print(\"Weights:\", lin.weight)\n",
        "print(\"Bias   :\", lin.bias)\n",
        "print(\"Output :\", lin(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a04f56bf",
      "metadata": {
        "id": "a04f56bf"
      },
      "source": [
        "### 2.2 - A `Transformer` Layer\n",
        "\n",
        "Most LLMs are a **stack of identical Transformer blocks**. Each block fuses two main components:\n",
        "\n",
        "| Step | What it does | Where it lives in code |\n",
        "|------|--------------|------------------------|\n",
        "| **Multi-Head Self-Attention** | Every token looks at every other token and decides *what matters*. | `block.attn` |\n",
        "| **Feed-Forward Network (MLP)** | Re-mixes information token-by-token. | `block.mlp` |\n",
        "\n",
        "Below, we load the smallest public GPT-2 (124 M parameters), grab its *first* block, and inspect the pieces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "47c87f6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47c87f6e",
        "outputId": "4720d97c-28d5-4893-e68d-4ddbee1e18c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer blocks: 12\n",
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the 124 M-parameter GPT-2 and inspect its layers (12 layers)\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "print(f\"transformer blocks: {len(gpt2.transformer.h)}\")\n",
        "print(gpt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "92df06df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92df06df",
        "outputId": "1bdc66ab-555d-4e5b-db0c-dffaf8e00a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dummy_tokens: tensor([[15146, 19661,  8034,  8775, 23466, 42949, 27245,  6790]])\n",
            "embeddings shape: torch.Size([1, 8, 768])\n",
            "embeddings: tensor([[[-0.0795, -0.2476,  0.1236,  ...,  0.1561, -0.0232,  0.0621],\n",
            "         [-0.1014, -0.0639,  0.0531,  ..., -0.1345,  0.1191, -0.1386],\n",
            "         [ 0.0513, -0.0099,  0.1009,  ...,  0.0795,  0.0765,  0.0175],\n",
            "         ...,\n",
            "         [ 0.0208, -0.2269,  0.1725,  ..., -0.1856, -0.3065,  0.1165],\n",
            "         [ 0.0332, -0.0548,  0.0652,  ..., -0.0984,  0.1579, -0.1325],\n",
            "         [ 0.0091, -0.0019,  0.1404,  ..., -0.2151, -0.0371, -0.1783]]])\n",
            "positions: tensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
            "position embeddings shape: torch.Size([1, 8, 768])\n",
            "position embeddings: tensor([[[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
            "           2.8267e-02,  5.4490e-02],\n",
            "         [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
            "           1.0172e-02, -1.5573e-04],\n",
            "         [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
            "           1.9325e-02, -2.1424e-02],\n",
            "         ...,\n",
            "         [ 9.6023e-03, -3.3885e-02,  1.3123e-01,  ...,  5.8940e-03,\n",
            "           7.1222e-03, -7.4742e-03],\n",
            "         [ 2.6788e-03, -2.0530e-02,  1.1961e-01,  ...,  2.4907e-03,\n",
            "           3.7071e-03, -2.5584e-03],\n",
            "         [ 2.5308e-03, -3.1787e-03,  1.1741e-01,  ...,  2.0096e-03,\n",
            "           4.4180e-03, -6.8326e-03]]])\n",
            "input embeddings shape: torch.Size([1, 8, 768])\n",
            "input embeddings: tensor([[[-0.0983, -0.4450,  0.1277,  ...,  0.1130,  0.0050,  0.1166],\n",
            "         [-0.0775, -0.1177, -0.0418,  ..., -0.1003,  0.1292, -0.1388],\n",
            "         [ 0.0555, -0.0946,  0.1555,  ...,  0.0993,  0.0959, -0.0039],\n",
            "         ...,\n",
            "         [ 0.0304, -0.2608,  0.3038,  ..., -0.1797, -0.2994,  0.1090],\n",
            "         [ 0.0359, -0.0753,  0.1848,  ..., -0.0959,  0.1616, -0.1350],\n",
            "         [ 0.0117, -0.0051,  0.2578,  ..., -0.2131, -0.0327, -0.1852]]])\n",
            "output shape: torch.Size([1, 8, 768])\n",
            "output: tensor([[[-0.5854, -3.3379,  1.2299,  ..., -1.9055, -1.1078, -0.4689],\n",
            "         [ 0.8870, -0.9648,  0.2668,  ...,  0.4006,  0.9878,  0.3746],\n",
            "         [ 0.2948, -0.7334,  0.2945,  ...,  0.3001, -0.0531, -1.4096],\n",
            "         ...,\n",
            "         [-0.1817, -2.7110, -2.2465,  ..., -2.2131, -0.8785,  1.4778],\n",
            "         [ 0.7931,  0.2334,  0.5901,  ..., -0.0812,  0.8803,  0.4239],\n",
            "         [ 0.5910,  1.1656,  3.2056,  ..., -2.6395, -0.9875, -0.3216]]])\n",
            "\n",
            "Output shape: torch.Size([1, 8, 768])\n"
          ]
        }
      ],
      "source": [
        "# Run a tiny forward pass through the first block\n",
        "seq_len = 8\n",
        "dummy_tokens = torch.randint(0, gpt2.config.vocab_size, (1, seq_len))\n",
        "print(\"dummy_tokens:\", dummy_tokens)  # a 1x8 tensor with 8 random integers in vocab size range\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Embed tokens + positions the same way GPT-2 does\n",
        "    # Forward through one layer\n",
        "\n",
        "    # tokens\n",
        "    token_embeddings = gpt2.transformer.wte(dummy_tokens)\n",
        "    print(\"embeddings shape:\", token_embeddings.shape)\n",
        "    print(\"embeddings:\", token_embeddings)\n",
        "\n",
        "    # positions\n",
        "    positions = torch.arange(seq_len, device=dummy_tokens.device).unsqueeze(0)\n",
        "    print(\"positions:\", positions)\n",
        "    position_embeddings = gpt2.transformer.wpe(positions)\n",
        "    print(\"position embeddings shape:\", position_embeddings.shape)\n",
        "    print(\"position embeddings:\", position_embeddings)\n",
        "\n",
        "    # combine\n",
        "    input_embeddings = token_embeddings + position_embeddings\n",
        "    print(\"input embeddings shape:\", input_embeddings.shape)\n",
        "    print(\"input embeddings:\", input_embeddings)\n",
        "\n",
        "    # pass through transformer\n",
        "    out = gpt2.transformer.h[0](input_embeddings)[0]\n",
        "    print(\"output shape:\", out.shape)\n",
        "    print(\"output:\", out)\n",
        "\n",
        "print(\"\\nOutput shape:\", out.shape)  # (batch, seq_len, hidden_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8493ecc8",
      "metadata": {
        "id": "8493ecc8"
      },
      "source": [
        "### 2.3 - Inside GPT-2\n",
        "\n",
        "GPT-2 is just many of those modules arranged in a repeating *block*. Let's print the modules inside the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a78ddee1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a78ddee1",
        "outputId": "cf310bbe-0944-4005-da56-c8dd716f403c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module name: transformer\n",
            "GPT2Model(\n",
            "  (wte): Embedding(50257, 768)\n",
            "  (wpe): Embedding(1024, 768)\n",
            "  (drop): Dropout(p=0.1, inplace=False)\n",
            "  (h): ModuleList(\n",
            "    (0-11): 12 x GPT2Block(\n",
            "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn): GPT2Attention(\n",
            "        (c_attn): Conv1D(nf=2304, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=768)\n",
            "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp): GPT2MLP(\n",
            "        (c_fc): Conv1D(nf=3072, nx=768)\n",
            "        (c_proj): Conv1D(nf=768, nx=3072)\n",
            "        (act): NewGELUActivation()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            ")\n",
            "--------------------\n",
            "Module name: lm_head\n",
            "Linear(in_features=768, out_features=50257, bias=False)\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Print the name and modules inside gpt2\n",
        "\n",
        "# print(\"do:\", gpt2.transformer.drop)\n",
        "for name, module in gpt2.named_children():\n",
        "    print(\"Module name:\", name)\n",
        "    print(module)\n",
        "    print(\"-\" * 20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed029847",
      "metadata": {
        "id": "ed029847"
      },
      "source": [
        "As you can see, the Transformer holds various modules, arranged from a list of blocks (`h`). The following table summarizes these modules:\n",
        "\n",
        "| Step | What it does | Why it matters |\n",
        "|------|--------------|----------------|\n",
        "| **Token â†’ Embedding** | Converts IDs to vectors | Gives the model a numeric â€œhandleâ€ on words |\n",
        "| **Positional Encoding** | Adds â€œwhere am I?â€ info | Order matters in language |\n",
        "| **Multi-Head Self-Attention** | Each token asks â€œwhich other tokens should I look at?â€ | Lets the model relate words across a sentence |\n",
        "| **Feed-Forward Network** | Two stacked Linear layers with a non-linearity | Mixes information and adds depth |\n",
        "| **LayerNorm & Residual** | Stabilize training and help gradients flow | Keeps very deep networks trainable |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6a7495",
      "metadata": {
        "id": "0a6a7495"
      },
      "source": [
        "### 2.4 LLM's output\n",
        "\n",
        "Passing a token sequence through an **LLM** yields a tensor of **logits** with shape  \n",
        "`(batch_size, seq_len, vocab_size)`.  \n",
        "Applying `softmax` on the last dimension turns those logits into probabilities.\n",
        "\n",
        "The cell below feeds an 8-token dummy sequence, prints the logits shape, and shows the five most likely next tokens for the final position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f98b7b34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98b7b34",
        "outputId": "2d9b9716-a44c-4054-fd8a-aa7c143392f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: tensor([[  40,  716,  262,  845, 2746,  286,  257, 3660, 1688]])\n",
            "output shape: CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ -39.3084,  -39.0100,  -41.8374,  ...,  -46.9337,  -44.9074,\n",
            "           -39.5149],\n",
            "         [-114.8939, -114.2495, -119.9518,  ..., -122.6206, -121.7605,\n",
            "          -117.6996],\n",
            "         [ -95.8772,  -93.9861,  -96.1859,  ...,  -98.2554,  -99.7003,\n",
            "           -94.3344],\n",
            "         ...,\n",
            "         [-105.7121, -103.5980, -108.6309,  ..., -110.3217, -110.9370,\n",
            "          -104.8304],\n",
            "         [ -93.1989,  -90.3098,  -98.2595,  ..., -100.0687, -101.5967,\n",
            "           -95.8216],\n",
            "         [ -75.4937,  -74.8921,  -82.1228,  ...,  -85.4231,  -85.5668,\n",
            "           -77.4899]]]), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None, cross_attentions=None)\n",
            "output: CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[ -39.3084,  -39.0100,  -41.8374,  ...,  -46.9337,  -44.9074,\n",
            "           -39.5149],\n",
            "         [-114.8939, -114.2495, -119.9518,  ..., -122.6206, -121.7605,\n",
            "          -117.6996],\n",
            "         [ -95.8772,  -93.9861,  -96.1859,  ...,  -98.2554,  -99.7003,\n",
            "           -94.3344],\n",
            "         ...,\n",
            "         [-105.7121, -103.5980, -108.6309,  ..., -110.3217, -110.9370,\n",
            "          -104.8304],\n",
            "         [ -93.1989,  -90.3098,  -98.2595,  ..., -100.0687, -101.5967,\n",
            "           -95.8216],\n",
            "         [ -75.4937,  -74.8921,  -82.1228,  ...,  -85.4231,  -85.5668,\n",
            "           -77.4899]]]), past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]), hidden_states=None, attentions=None, cross_attentions=None)\n",
            "Logits shape : torch.Size([1, 9, 50257])\n",
            "Last token logits shape : torch.Size([50257])\n",
            "Last token logits : tensor([-75.4937, -74.8921, -82.1228,  ..., -85.4231, -85.5668, -77.4899])\n",
            "Probabilities shape : torch.Size([50257])\n",
            "Probabilities : tensor([2.8245e-04, 5.1548e-04, 3.7322e-07,  ..., 1.3762e-08, 1.1919e-08,\n",
            "        3.8373e-05])\n",
            "Top-5 tokens : torch.return_types.topk(\n",
            "values=tensor([0.3158, 0.0818, 0.0375, 0.0258, 0.0215]),\n",
            "indices=tensor([4652,   12,   13,   11,  995]))\n",
            "\n",
            "Top-5 predictions for the next token:\n",
            "prediction:  league , probability: tensor(0.3158)\n",
            "prediction: - , probability: tensor(0.0818)\n",
            "prediction: . , probability: tensor(0.0375)\n",
            "prediction: , , probability: tensor(0.0258)\n",
            "prediction:  world , probability: tensor(0.0215)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "# Load gpt2 model and tokenizer\n",
        "\n",
        "# load the model\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "# disable dropouts since we're just doing inference\n",
        "gpt2.eval()\n",
        "\n",
        "# Tokenize input text\n",
        "text = \"I am the very model of a modern major\"\n",
        "# tokenize the input, get a pytorch tensor as output to be used in next step (instead of a simple list of tokens)\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(\"tokens:\", tokens)\n",
        "\n",
        "# Get logits by passing the ids to the gpt2 model.\n",
        "# disable gradient tracking since we don't need backprop for inference\n",
        "with torch.no_grad():\n",
        "    # execute the model using the tokens\n",
        "    output = gpt2(tokens)\n",
        "    print(\"output shape:\", output)\n",
        "    print(\"output:\", output)\n",
        "\n",
        "    # get the logistic units from output (logits) to use as input to softmax\n",
        "    logits = output.logits\n",
        "    print(\"Logits shape :\", logits.shape)\n",
        "\n",
        "# Predict next token\n",
        "# each element is the logits representing what gpt2 would predict as the NEXT token if that index was the last of the input given\n",
        "# so taking the logits of the LAST token of output gives us the data needed to calculate (using softmax) the next token prediction\n",
        "# of the entire input sequence\n",
        "last_token_logits = logits[0, -1, :]\n",
        "print(\"Last token logits shape :\", last_token_logits.shape)\n",
        "print(\"Last token logits :\", last_token_logits)\n",
        "\n",
        "# take the last token logits and give it to softmax, which will give us the probabilities of each of the possible elements of the\n",
        "# vocab (the logits won't necessarily add up to 1, but the softmax result will)\n",
        "probs = F.softmax(last_token_logits, dim=-1)\n",
        "print(\"Probabilities shape :\", probs.shape)\n",
        "print(\"Probabilities :\", probs)\n",
        "\n",
        "# just pick out the top 5 most likely next token indices (as well as their actual probabilities)\n",
        "top_5_tokens = torch.topk(probs, 5)\n",
        "print(\"Top-5 tokens :\", top_5_tokens)\n",
        "\n",
        "print(\"\\nTop-5 predictions for the next token:\")\n",
        "# iterate over top 5 most likely next tokens and output them with actual probability scores\n",
        "for i in range(len(top_5_tokens.indices)):\n",
        "    print(\"prediction:\", tokenizer.decode(top_5_tokens.indices[i]), \", probability:\", top_5_tokens.values[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb05c9b",
      "metadata": {
        "id": "0eb05c9b"
      },
      "source": [
        "### 2.5 - Key Takeaway\n",
        "\n",
        "A language model is nothing mystical: itâ€™s a *huge composition* of small, understandable layers trained to predict the next token in a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ccf391",
      "metadata": {
        "id": "e0ccf391"
      },
      "source": [
        "# 3 - Generation\n",
        "Once an LLM is trained to predict the probabilities, we can generate text from the model. This process is called decoding or sampling.\n",
        "\n",
        "At each step, the LLM outputs a **probability distribution** over the next token. It is the job of the decoding algorithm to pick the next token, and move on to the next token. There are different decoding algorithms and hyper-parameters to control the generaiton:\n",
        "* **Greedy** â†’ pick the single highestâ€‘probability token each step (safe but repetitive).  \n",
        "* **Topâ€‘k / Nucleus (topâ€‘p)** â†’ sample from a subset of likely tokens (adds variety).\n",
        "* **beam** -> applies beam search to pick tokens\n",
        "* **Temperature** â†’ a *creativity* knob. Higher values flatten the probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0c5728",
      "metadata": {
        "id": "ac0c5728"
      },
      "source": [
        "### 3.1 - Greedy decoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "}\n",
        "tokenizers, models = {}, {}\n",
        "\n",
        "# Load models and tokenizers\n",
        "for k, v in MODELS.items():\n",
        "    tokenizers[k] = AutoTokenizer.from_pretrained(v)\n",
        "    models[k] = AutoModelForCausalLM.from_pretrained(v)\n"
      ],
      "metadata": {
        "id": "It5IRfx5EO_J",
        "outputId": "a6dc3c25-3eec-4801-f461-6b42a8b46d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "id": "It5IRfx5EO_J",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-740059426.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m MODELS = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m }\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .utils import (\n\u001b[1;32m     29\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .auto_docstring import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mClassAttrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mClassDocstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/auto_docstring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0m_prepare_output_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# required for @can_return_tuple decorator to work with torchdynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_debugging_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_addition_debugger_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mUninitializedParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# usort: skip # noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from torch.nn import (\n\u001b[1;32m     10\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBilinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m from .activation import (\n\u001b[1;32m      4\u001b[0m     \u001b[0mCELU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mELU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceLikeType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackwardHook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRemovableHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from torch.utils import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbackcompat\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbackcompat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcollect_env\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcollect_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from torch.utils.data.dataloader import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdefault_collate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdefault_convert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/graph_settings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from torch.utils.data.datapipes.iter.sharding import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0m_ShardingIterDataPipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mSHARDING_PRIORITIES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/datapipes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/datapipes/iter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from torch.utils.data.datapipes.iter.callable import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mCollatorIterDataPipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mCollator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mMapperIterDataPipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m from torch.utils.data.datapipes.iter.combinatorics import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/datapipes/iter/callable.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decorator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_datapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataframe_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdf_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal_handling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2f2cb953",
      "metadata": {
        "id": "2f2cb953"
      },
      "outputs": [],
      "source": [
        "def generate(model_key, prompt, strategy=\"greedy\", max_new_tokens=100):\n",
        "    tok, mdl = tokenizers[model_key], models[model_key]\n",
        "    # Return the generations based on the provided strategy: greedy, top_k, top_p\n",
        "    with torch.no_grad():\n",
        "        print(\"input:\", prompt)\n",
        "        for i in range(max_new_tokens):\n",
        "            tokens = tok.encode(prompt, return_tensors=\"pt\")\n",
        "            output = mdl(tokens)\n",
        "            logits = output.logits\n",
        "            last_token_logits = logits[0, -1, :]\n",
        "\n",
        "            if strategy == \"greedy\":\n",
        "                next_token = F.softmax(last_token_logits, dim=-1).argmax().item()\n",
        "                next_token_decoded = tok.decode(next_token)\n",
        "            elif strategy == \"top-p\":\n",
        "                p = 0.9\n",
        "                sorted_logits, sorted_indices = torch.sort(last_token_logits, descending=True)\n",
        "                sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "                keep_mask = cumulative_probs <= p  # [True,True,...,False,False]\n",
        "                keep_mask[0] = True  # always keep the most likely token\n",
        "                filtered_probs = sorted_probs * keep_mask\n",
        "                filtered_probs = filtered_probs / filtered_probs.sum()  # renormalize\n",
        "\n",
        "                sampled_idx = torch.multinomial(filtered_probs, num_samples=1).item()\n",
        "                next_token_decoded = tok.decode(sorted_indices[sampled_idx])\n",
        "            elif strategy == \"top-k\":\n",
        "                k = 10\n",
        "                sorted_logits, sorted_indices = torch.sort(last_token_logits, descending=True)\n",
        "                sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "                top_k_tokens = torch.topk(sorted_probs, k=k)\n",
        "\n",
        "                # renormalize top k and then pick based on probabilities\n",
        "                top_k_probs = top_k_tokens.values / top_k_tokens.values.sum()\n",
        "                sampled_idx = torch.multinomial(top_k_probs, num_samples=1).item()\n",
        "                next_token_decoded = tok.decode(top_k_tokens.indices[sampled_idx])\n",
        "\n",
        "            prompt += next_token_decoded\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "dbe777ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbe777ba",
        "outputId": "ad3a269e-27bd-4fb2-ae61-1436a490b94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== GPT-2 | Greedy ==\n",
            "input: Once upon a time\n",
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and\n",
            "\n",
            "== GPT-2 | Greedy ==\n",
            "input: What is 2+2?\n",
            "What is 2+2?\n",
            "\n",
            "\n",
            "2+2 is a term used to describe a number that is greater than or equal to 2.\n",
            "\n",
            "\n",
            "2+2 is a number that is greater than or equal to 2.\n",
            "\n",
            "\n",
            "2+2 is a number that is greater than or equal to 2.\n",
            "\n",
            "\n",
            "2+2 is a number that is greater than or equal to 2.\n",
            "\n",
            "\n",
            "2+2 is\n",
            "\n",
            "== GPT-2 | Greedy ==\n",
            "input: Suggest a party theme.\n",
            "Suggest a party theme.\n",
            "\n",
            "\n",
            "The party theme is a simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple, simple,\n"
          ]
        }
      ],
      "source": [
        "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Greedy ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"greedy\", 80))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51d44b2",
      "metadata": {
        "id": "f51d44b2"
      },
      "source": [
        "\n",
        "Naively picking the single best token every time has the following issues in practice:\n",
        "\n",
        "* **Loop**: â€œThe cat is is isâ€¦â€  \n",
        "* **Miss long-term payoff**: the highest-probability word *now* might paint you into a boring corner later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91607661",
      "metadata": {
        "id": "91607661"
      },
      "source": [
        "### 3.2 - Top-k or top-p sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0633d4a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0633d4a3",
        "outputId": "32865577-dfe0-4054-b6ee-fd2cb005945f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== GPT-2 | Top-p ==\n",
            "input: Once upon a time\n",
            "Once upon a time the sisters were your mortal people, to your birth-stricken ancestors, even the safest heart could be your weakest and most dangerous enemy. However, those who had loved those loves much enough to help\n",
            "\n",
            "== GPT-2 | Top-p ==\n",
            "input: What is 2+2?\n",
            "What is 2+2?\n",
            "\n",
            "\n",
            "A+2, any of which counts as value the p value, etc. Example:\n",
            "\n",
            "\n",
            "-On the problem is that the keyboard tends to be faster than a single button,\n",
            "\n",
            "== GPT-2 | Top-p ==\n",
            "input: Suggest a party theme.\n",
            "Suggest a party theme.\n",
            "\n",
            "\n",
            "Create your party hosts and teams using either American Legion club addresses, or World of Warcraft based address styles.\n",
            "\n",
            "\n",
            "Features List\n",
            "\n",
            "\n",
            "FAQ [ edit ]\n",
            "\n",
            "\n",
            "Q:\n"
          ]
        }
      ],
      "source": [
        "tests=[\"Once upon a time\",\"What is 2+2?\", \"Suggest a party theme.\"]\n",
        "for prompt in tests:\n",
        "    print(f\"\\n== GPT-2 | Top-p ==\")\n",
        "    print(generate(\"gpt2\", prompt, \"top-p\", 40))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004b4039",
      "metadata": {
        "id": "004b4039"
      },
      "source": [
        "### 3.3 - Try It Yourself\n",
        "\n",
        "1. Scroll to the list called `tests`.\n",
        "2. Swap in your own prompts or tweak the decoding strategy.  \n",
        "3. Reâ€‘run the cell and compare the vibes.\n",
        "\n",
        "> **Tip:** Try the same prompt with `greedy` vs. `top_p` (0.9) and see how the tone changes. Notice especially how small temperature tweaks can soften or sharpen the prose.\n",
        "\n",
        "* `strategy`: `\"greedy\"`, `\"beam\"`, `\"top_k\"`, `\"top_p\"`  \n",
        "* `temperature`: `0.2 â€“ 2.0`  \n",
        "* `k` or `p` thresholds\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b775b02",
      "metadata": {
        "id": "6b775b02"
      },
      "source": [
        "# 4 - Completion vs. Instruction-tuned LLMs\n",
        "\n",
        "We have seen that we can use GPT2 model to pass an input text and generate a new text. However, this model only continues the provided text. It is not engaging in a dialouge-like conversation and cannot be helpful by answering instructions. On the other hand, **instruction-tuned LLMs** like `Qwen-Chat` go through an extra training stage called **post-training** after the base â€œcompletionâ€ model is finished. Because of post-training step, an instruction-tuned LLM will:\n",
        "\n",
        "* **Read the entire prompt as a request,** not just as text to mimic.  \n",
        "* **Stay in dialogue mode**. Answer questions, follow steps, ask clarifying queries.  \n",
        "* **Refuse or safe-complete** when instructions are unsafe or disallowed.  \n",
        "* **Adopt a consistent persona** (e.g., â€œAssistantâ€) rather than drifting into story continuation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1706dc08",
      "metadata": {
        "id": "1706dc08"
      },
      "source": [
        "### 4.1 - Qwen1.5-8B vs. GPT2\n",
        "\n",
        "In the code below weâ€™ll feed the same prompt to:\n",
        "\n",
        "* **GPT-2 (completion-only)** â€“ it will simply keep writing in the same style.  \n",
        "* **Qwen-Chat (instruction-tuned)** â€“ it will obey the instruction and respond directly.\n",
        "\n",
        "Comparing the two outputs makes the difference easy to see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b73e7a",
      "metadata": {
        "id": "57b73e7a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "    \"qwen\": \"Qwen/Qwen1.5-1.8B-Chat\"\n",
        "}\n",
        "tokenizers, models = {}, {}\n",
        "\n",
        "# Load models and tokenizers\n",
        "for k, v in MODELS.items():\n",
        "    tokenizers[k] = AutoTokenizer.from_pretrained(v)\n",
        "    models[k] = AutoModelForCausalLM.from_pretrained(v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef49ab1b",
      "metadata": {
        "id": "ef49ab1b"
      },
      "source": [
        "\n",
        "We downloaded two tiny checkpoints: `GPTâ€‘2` (124â€¯M parameters) and `Qwenâ€‘1.5â€‘Chat` (1.8â€¯B). If the cell took a while, that was mostly network time. Models are stored locally after the first run.\n",
        "\n",
        "Let's now generate text and compare two models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0c78a508",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "0c78a508",
        "outputId": "be4436d1-b2a5-4f79-ad18-9f208cce42e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== GPT2 | top-k ==\n",
            "input:  What is 2+2?\n",
            "What is 2+2?!!!!&\"\"*#!&*!!\"\"$#'\"&&&!$!$\"!#'\"#!'\"!!*)&!!&!(\"#\"$$#!$#\"!\"!\"!\"!$\"!\"!!($%!!)##\"#\"\n",
            "\n",
            "== QWEN | top-k ==\n",
            "input:  What is 2+2?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3411312614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"qwen\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n== {key.upper()} | {strategy} ==\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3981952005.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model_key, prompt, strategy, max_new_tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlast_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "tests=[(\"Once upon a time\",\"greedy\"),(\"What is 2+2?\",\"top_k\"),(\"Suggest a party theme.\",\"top_p\")]\n",
        "for prompt,strategy in tests:\n",
        "    for key in [\"gpt2\",\"qwen\"]:\n",
        "        print(f\"\\n== {key.upper()} | {strategy} ==\")\n",
        "        print(generate(key,prompt,strategy,80))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1c3da1",
      "metadata": {
        "id": "8e1c3da1"
      },
      "source": [
        "# 5. (Optional) A Small LLM Playground"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313ba974",
      "metadata": {
        "id": "313ba974"
      },
      "source": [
        "### 5.1â€¯â€‘ Interactive Playground\n",
        "\n",
        "Enter a prompt, pick a model and decoding strategy, adjust the temperature, and press **Generate** to watch the model respond.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a67a884",
      "metadata": {
        "id": "1a67a884"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Make sure models and tokenizers are loaded\n",
        "try:\n",
        "    tokenizers\n",
        "    models\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Please run the earlier setup cells that load the models before using the playground.\")\n",
        "\n",
        "def generate_playground(model_key, prompt, strategy=\"greedy\", temperature=1.0, max_new_tokens=100):\n",
        "    # Generation code\n",
        "    \"\"\"\n",
        "    YOUR CODE HERE\n",
        "    \"\"\"\n",
        "\n",
        "# Your code to build boxes, dropdowns, and other elements in the UI using widgets and creating the UI using widgets.vbox and display.\n",
        "# Refer to https://ipywidgets.readthedocs.io/en/stable/\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbccead",
      "metadata": {
        "id": "cfbccead"
      },
      "source": [
        "\n",
        "## ðŸŽ‰ Congratulations!\n",
        "\n",
        "Youâ€™ve just learned, explored, and inspected a real **LLM**. In one project you:\n",
        "* Learned how **tokenization** works in practice\n",
        "* Used `tiktoken` library to load and experiment with most advanced tokenizers.\n",
        "* Explored LLM architecture and inspected GPT2 blocks and layers\n",
        "* Learned decoding strategies and used `top-p` to generate text from GPT2\n",
        "* Loaded a powerful chat model, `Qwen1.5-8B` and generated text\n",
        "* Built an LLM playground\n",
        "\n",
        "\n",
        "ðŸ‘ **Great job!** Take a moment to celebrate. You now have a working mental model of how LLMs work. The skills you used here power most LLMs you see everywhere.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30824bd6",
      "metadata": {
        "id": "30824bd6"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_playground",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}